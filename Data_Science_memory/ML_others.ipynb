{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # для матричных вычислений\n",
    "import pandas as pd # для анализа и предобработки данных\n",
    "import matplotlib.pyplot as plt # для визуализации\n",
    "import seaborn as sns # для визуализации\n",
    "\n",
    "from sklearn import linear_model # линейные модели\n",
    "from sklearn import metrics # метрики\n",
    "\n",
    "from sklearn.model_selection import train_test_split # сплитование выборки\n",
    "from sklearn import preprocessing # предобработка\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим данные для образца\n",
    "from sklearn.datasets import load_boston \n",
    "\n",
    "boston = load_boston()\n",
    "# создаём DataFrame из загруженных numpy-матриц\n",
    "boston_data = pd.DataFrame(\n",
    "    data=boston.data, #данные\n",
    "    columns=boston.feature_names #наименования столбцов\n",
    ")\n",
    "# добавляем в таблицу столбец с целевой переменной\n",
    "boston_data['MEDV'] = boston.target\n",
    "boston_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston_data.drop('MEDV', axis=1) #матрица наблюдений\n",
    "y = boston_data['MEDV'] #вектор правильных ответов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# РАЗДЕЛЕНИЕ выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Разделяем выборку на тренировочную и тестовую в соотношении 70/30\n",
    "\n",
    "# Устанавливаем random_state для воспроизводимости результатов \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False, stratify=y, random_state=40)\n",
    "# X - матрица признаков (наблюдений)\n",
    "# y - вектор правильных ответов\n",
    "# test_size=0.3 - 30% данных идут в тестовую выборку\n",
    "# shuffle=False - По умолчанию True. Параметр перемешивания данных в выборке.\n",
    "# stratify=y - Стратифицированное разбиение - Одинаковое соотношение целевого признака \n",
    "# в тренировочной и тестовой выборке, не допускающее перекоса в обучении модели.\n",
    "# random_state=40 - гарантирует выдачу генератором одних и тех же случайных значений\n",
    "# \n",
    "\n",
    "\n",
    "# Выводим результирующие размеры таблиц\n",
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Test:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# МЕТРИКИ РЕГРЕССИИ\n",
    "см ML2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Делаем предсказание по всем признакам\n",
    "y_predict_full = lr_full.predict(boston_data[features])\n",
    "# Рассчитываем MAE - Средняя абсолютная ошибка\n",
    "print('MAE score: {:.3f} thou. $'.format(metrics.mean_absolute_error(y, y_predict_full)))\n",
    "# Рассчитываем RMSE - Корень из средней квадратической ошибки\n",
    "print('RMSE score: {:.3f} thou. $'.format(np.sqrt(metrics.mean_squared_error(y, y_predict_full))))\n",
    "# Рассчитываем MAPE - Средняя абсолютная ошибка в процентах\n",
    "print('MAPE score: {:.3f} %'.format(metrics.mean_absolute_percentage_error(y, y_predict_full) * 100))\n",
    "# Рассчитываем коэффициент детерминации Коэффициент детерминации (R2)\n",
    "print('R2 score: {:.3f}'.format(metrics.r2_score(y, y_predict_full)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# МЕТРИКИ КЛАССИФИКАЦИИ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy** (достоверность) — доля правильных ответов модели среди всех ответов. \n",
    "* Интерпретация: как много (в долях) модель угадала ответов.\n",
    "* Accuracy — самая простая и самая понятная метрика классификации, но у неё есть один существенный недостаток. \n",
    "* Она бесполезна, если классы сильно несбалансированы.\n",
    "\n",
    "**Precision** (точность), или PPV (Positive Predictive Value) — это доля объектов, которые действительно являются положительными, по отношению ко всем объектам, названным моделью положительными.\n",
    "* Интерпретация: способность отделить класс 1 от класса 0. Чем больше precision, тем меньше ложных попаданий. \n",
    "* Precision нужен в задачах, где от нас требуется минимум ложных срабатываний. Чем выше «цена» ложноположительного результата, тем выше должен быть precision.\n",
    "* Можно использовать на несбалансированных выборках.\n",
    "\n",
    "**Recall** (полнота), или TPR (True Positive Rate) — это доля объектов, названных классификатором положительными, по отношению ко всем объектам положительного класса.\n",
    "* Интерпретация: способность модели обнаруживать класс 1 вообще, то есть охват класса 1. Заметьте, что ложные срабатывания не влияют на recall. \n",
    "* Recall очень хорошо себя показывает в задачах, где важно найти как можно больше объектов, принадлежащих к классу 1.\n",
    "* Можно использовать на несбалансированных выборках.\n",
    "\n",
    "\n",
    "Концентрация только на одной метрике (precision или recall) без учёта второй — сомнительная идея.\n",
    "В битве за максимум precision для класса 1 побеждает модель, которая всегда будет говорить говорить «нет». У неё вообще не будет ложных срабатываний.\n",
    "В битве за максимум recall для класса 1 побеждает модель, которая всегда будет говорить «да». Она охватит все наблюдения класса 1. \n",
    "В реальности необходимо балансировать между двумя этими метриками.\n",
    "\n",
    "**F1** (F-мера) — это взвешенное среднее гармоническое между precision и recall:\n",
    "* Несмотря на отсутствие бизнес-интерпретации, метрика F1 является довольно распространённой и используется в задачах, где необходимо выбрать модель, которая балансирует между precision и recall.\n",
    "* Используется в задачах, где необходимо балансировать между precision и recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Модель log_reg_full:\n",
    "#Рассчитываем accuracy\n",
    "print('Accuracy: {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
    "#Рассчитываем precision\n",
    "print('Precision: {:.2f}'.format(metrics.precision_score(y, y_pred)))\n",
    "#Рассчитываем recall\n",
    "print('Recall: {:.2f}'.format(metrics.recall_score(y, y_pred)))\n",
    "#Рассчитываем F1-меру\n",
    "print('F1 score: {:.2f}'.format(metrics.f1_score(y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метрики классификации в одной строке\n",
    "print(metrics.classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что отображено в print(metrics.classification_report(y, y_pred))?\n",
    "\n",
    "1) В первой части таблицы отображаются метрики precision, recall и f1-score, рассчитанные для каждого класса в отдельности. Столбец support — это количество объектов каждого из классов.\n",
    "2) Во второй части таблицы отображена общая метрика accuracy. \n",
    "3) Далее идёт строка macro avg — это среднее значение метрики между классами 1 и 0. Например, значение в строке macro avg и столбце recall = (0.88 + 0.56)/2=0.72.\n",
    "4) Завершает отчёт строка weighted avg — это средневзвешенное значение метрики между классами 1 и 0. Рассчитывается по формуле:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нас интересует только вероятность класса (второй столбец)\n",
    "y_test_proba_pred = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "# Для удобства завернем numpy-массив в pandas Series\n",
    "y_test_proba_pred = pd.Series(y_test_proba_pred)\n",
    "# Создадим списки, в которых будем хранить значения метрик \n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "f1_scores = []\n",
    "# Сгенерируем набор вероятностных порогов в диапазоне от 0.1 до 1\n",
    "thresholds = np.arange(0.1, 1, 0.05)\n",
    "# В цикле будем перебирать сгенерированные пороги\n",
    "for threshold in thresholds:\n",
    "    # Пациентов, для которых вероятность наличия диабета > threshold относим к классу 1\n",
    "    # В противном случае - к классу 0\n",
    "    y_test_pred = y_test_proba_pred.apply(lambda x: 1 if x>threshold else 0)\n",
    "    # Считаем метрики и добавляем их в списки\n",
    "    recall_scores.append(metrics.recall_score(y_test, y_test_pred))\n",
    "    precision_scores.append(metrics.precision_score(y_test, y_test_pred))\n",
    "    f1_scores.append(metrics.f1_score(y_test, y_test_pred))\n",
    "\n",
    "# Визуализируем метрики при различных threshold\n",
    "fig, ax = plt.subplots(figsize=(10, 4)) #фигура + координатная плоскость\n",
    "# Строим линейный график зависимости recall от threshold\n",
    "ax.plot(thresholds, recall_scores, label='Recall')\n",
    "# Строим линейный график зависимости precision от threshold\n",
    "ax.plot(thresholds, precision_scores, label='Precision')\n",
    "\n",
    "# Строим линейный график зависимости F1 от threshold\n",
    "ax.plot(thresholds, f1_scores, label='F1-score')\n",
    "# Даем графику название и подписи осям\n",
    "ax.set_title('Recall/Precision dependence on the threshold')\n",
    "ax.set_xlabel('Probability threshold')\n",
    "ax.set_ylabel('Score')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задаем оптимальный порог вероятностей\n",
    "threshold_opt = 0.4 # полученное по графику значение\n",
    "# Людей, у которых вероятность зарабатывать >50K больше 0.5 относим к классу 1\n",
    "# В противном случае - к классу 0\n",
    "y_test_pred_opt = y_test_proba_pred.apply(lambda x: 1 if x > threshold_opt else 0)\n",
    "# Считаем метрики\n",
    "print(metrics.classification_report(y_test, y_test_pred_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# АНАЛИТИЧЕСКОЕ РЕШЕНИЕ \"РЕГРЕССИИ\" С ПОМОЩЬЮ NUMPY\n",
    "см ML2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y):\n",
    "    # Создаём вектор из единиц\n",
    "    ones = np.ones(X.shape[0])\n",
    "    # Добавляем вектор к таблице первым столбцом\n",
    "    X = np.column_stack([ones, X])\n",
    "    # Вычисляем обратную матрицу Q\n",
    "    Q = np.linalg.inv(X.T @ X)\n",
    "    # Вычисляем вектор коэффициентов\n",
    "    w = Q @ X.T @ y\n",
    "    return w\n",
    "# Вычисляем параметры линейной регрессии\n",
    "w = linear_regression(X, y)\n",
    "# Выводим вычисленные значения параметров в виде вектора\n",
    "print('Vector w: {}'.format(w))\n",
    "# Выводим параметры с точностью до двух знаков после запятой\n",
    "print('w0 = {:.2f}'.format(w[0]))\n",
    "print('w1 = {:.2f}'.format(w[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# НАЛИТИЧЕСКОЕ РЕШЕНИЕ \"РЕГРЕССИИ\" С ПОМОЩЬЮ SKLEARN\n",
    "см ML2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём объект класса LinearRegression\n",
    "lr_full = linear_model.LinearRegression()\n",
    "\n",
    "# Обучаем модель — ищем параметры по МНК\n",
    "lr_full.fit(X, y) \n",
    "\n",
    "# Выводим полученные параметры\n",
    "print('w0 = {}'.format(lr_full.intercept_)) #свободный член w0\n",
    "print('w1 = {}'.format(lr_full.coef_)) #остальные параметры модели w1, w2, ..., wm\n",
    "\n",
    "# Предсказываем медианную цену для всех участков из набора данных\n",
    "y_predict = lr_full.predict(X)\n",
    "\n",
    "# Выводим предсказание\n",
    "# Составляем таблицу из признаков и их коэффициентов\n",
    "w_df = pd.DataFrame({'Features': X.columns, 'Coefficients': lr_full.coef_})\n",
    "# Составляем строку таблицы со свободным членом\n",
    "intercept_df =pd.DataFrame({'Features': ['INTERCEPT'], 'Coefficients': lr_full.intercept_})\n",
    "coef_df = pd.concat([w_df, intercept_df], ignore_index=True)\n",
    "display(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЧИСЛЕННОЕ РЕШЕНИЕ \"РЕГРЕССИИ\" С ПОМОЩЬЮ SKLEARN (SGD - градиентный спуск)\n",
    "см ML2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём объект класса линейной регрессии с SGD\n",
    "sgd_lr_full = linear_model.SGDRegressor(random_state=42)\n",
    "# У класса SGDRegressor, помимо random_state, есть ещё множество различных внешних параметров, \n",
    "# которые можно настраивать. Со всем списком вы можете ознакомиться в документации. \n",
    "# А мы приведём несколько самых важных:\n",
    "\n",
    "# loss — функция потерь. По умолчанию используется squared_loss — уже привычная нам MSE. \n",
    "# Но могут использоваться и несколько других. Например, значение \"huber\" определяет функцию потерь Хьюбера. \n",
    "# Эта функция менее чувствительна к наличию выбросов, чем MSE.\n",
    "\n",
    "# max_iter — максимальное количество итераций, выделенное на сходимость. Значение по умолчанию — 1000.\n",
    "\n",
    "# learning_rate — режим управления темпом обучения. Значение по умолчанию — 'invscaling'. \n",
    "# Этот режим уменьшает темп обучения по формуле, которую мы рассматривали ранее: etat=eta0/t^p.\n",
    "# Есть ещё несколько режимов управления, о которых вы можете прочитать в документации.\n",
    "# Если вы не хотите, чтобы темп обучения менялся на протяжении всего обучения, \n",
    "# то можете выставить значение параметра на \"constant\".\n",
    "\n",
    "# eta0 — начальное значение темпа обучения . Значение по умолчанию — 0.01.\n",
    "# Если параметр learning_rate=\"constant\", то значение этого параметра \n",
    "# будет темпом обучения на протяжении всех итераций.\n",
    "\n",
    "# power_t — значение мощности уменьшения  в формуле etat=eta0/t^p . Значение по умолчанию — 0.25.\n",
    "\n",
    "\n",
    "# Обучаем модель — ищем параметры по методу SGD\n",
    "sgd_lr_full.fit(X, y)\n",
    "\n",
    "# Выводим полученные параметры\n",
    "print('w0: {}'.format(sgd_lr_full.intercept_)) #свободный член w0\n",
    "print('w1: {}'.format(sgd_lr_full.coef_)) #остальные параметры модели w1, w2, ..., wm\n",
    "\n",
    "#Предсказываем медианную цену для всех участков из набора данных\n",
    "y_predict = sgd_lr_full.predict(X)\n",
    "\n",
    "# Выводим предсказание\n",
    "# Составляем таблицу из признаков и их коэффициентов\n",
    "w_df = pd.DataFrame({'Features': X.columns, 'Coefficients': lr_full.coef_})\n",
    "# Составляем строку таблицы со свободным членом\n",
    "intercept_df =pd.DataFrame({'Features': ['INTERCEPT'], 'Coefficients': lr_full.intercept_})\n",
    "coef_df = pd.concat([w_df, intercept_df], ignore_index=True)\n",
    "display(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# СМЕЩЕНИЕ (НЕДООБУЧЕНИЕ) И РАЗБРОС (ПЕРЕОБУЧЕНИЕ)\n",
    "см ML2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Недообучение (underfitting) — проблема, обратная переобучению. Модель из-за своей слабости не уловила никаких закономерностей в данных. В этом случае ошибка будет высокой как для тренировочных данных, так и для данных, не показанных во время обучения.\n",
    "\n",
    "Смещение (bias) — это математическое ожидание разности между истинным ответом и ответом, выданным моделью. То есть это ожидаемая ошибка модели.\n",
    "\n",
    "Разброс (variance) — это вариативность ошибки, то, насколько ошибка будет отличаться, если обучать модель на разных наборах данных. Математически это дисперсия (разброс) ответов модели.\n",
    "\n",
    "Регуляризация — способ уменьшения переобучения моделей машинного обучения.\n",
    "\n",
    "Штраф — это дополнительное неотрицательное слагаемое в выражении для функции потерь, которое специально повышает ошибку.  За счёт этого слагаемого метод оптимизации (OLS или SGD) будет находить не истинный минимум функции потерь, а псевдоминимум."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стандартизацию (нормализацию) полезнее проводить перед \n",
    "генерацией полиномиальных признаков, иначе можно потерять масштаб полиномов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем стандартизатор StandardScaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Подгоняем параметры стандартизатора (вычисляем среднее и СКО)\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Производим стандартизацию тренировочной выборки\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Производим стандартизацию тестовой выборки\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём генератор полиномиальных признаков\n",
    "poly = preprocessing.PolynomialFeatures(degree=2, include_bias=False)\n",
    "# degree=2 - степень полинома\n",
    "\n",
    "# include_bias=False — включать ли в результирующую таблицу столбец из единиц (x в степени 0). \n",
    "# По умолчанию стоит True, но лучше выставить его в значение False, так как столбец из единиц и так \n",
    "# добавляется в методе наименьших квадратов.\n",
    "\n",
    "# Подгоняем параметры\n",
    "poly.fit(X_train_scaled)\n",
    "\n",
    "# Генерируем полиномиальные признаки для тренировочной выборки\n",
    "X_train_scaled_poly = poly.transform(X_train_scaled)\n",
    "\n",
    "# Генерируем полиномиальные признаки для тестовой выборки\n",
    "X_test_scaled_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "# Выводим результирующие размерности таблиц \n",
    "print(X_train_scaled_poly.shape)\n",
    "print(X_test_scaled_poly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1-регуляризация (Lasso) \n",
    "# Создаём объект класса линейной регрессии с L1-регуляризацией\n",
    "lasso_lr_poly = linear_model.Lasso(alpha=0.1)\n",
    "# Главный параметр инициализации Lasso — это alpha, коэффициент регуляризации. \n",
    "# По умолчанию alpha=1. Практика показывает, что это довольно сильная регуляризация для L1-метода. \n",
    "# Давайте установим значение этого параметра на 0.1.\n",
    "\n",
    "# Обучаем модель\n",
    "lasso_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "\n",
    "# Делаем предсказание для тренировочной выборки\n",
    "y_train_predict_poly = lasso_lr_poly.predict(X_train_scaled_poly)\n",
    "# Делаем предсказание для тестовой выборки\n",
    "y_test_predict_poly = lasso_lr_poly.predict(X_test_scaled_poly)\n",
    "\n",
    "# Рассчитываем коэффициент детерминации для двух выборок\n",
    "print(\"Train R^2: {:.3f}\".format(metrics.r2_score(y_train, y_train_predict_poly)))\n",
    "print(\"Test R^2: {:.3f}\".format(metrics.r2_score(y_test, y_test_predict_poly)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2-регуляризация (Ridge)\n",
    "# Создаём объект класса линейной регрессии с L2-регуляризацией\n",
    "ridge_lr_poly = linear_model.Ridge(alpha=10)\n",
    "# Для L2-регуляризации параметр alpha по умолчанию равен 1. \n",
    "# Давайте попробуем использовать значение параметра alpha=10:\n",
    "\n",
    "# Обучаем модель\n",
    "ridge_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "\n",
    "# Делаем предсказание для тренировочной выборки\n",
    "y_train_predict_poly = ridge_lr_poly.predict(X_train_scaled_poly)\n",
    "# Делаем предсказание для тестовой выборки\n",
    "y_test_predict_poly = ridge_lr_poly.predict(X_test_scaled_poly)\n",
    "\n",
    "# Рассчитываем коэффициент детерминации для двух выборок\n",
    "print(\"Train R^2: {:.3f}\".format(metrics.r2_score(y_train, y_train_predict_poly)))\n",
    "print(\"Test R^2: {:.3f}\".format(metrics.r2_score(y_test, y_test_predict_poly)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ЛУЧШЕ ТА, У КОТОРОЙ ВЫШЕ ПОКАЗАТЕЛИ НА ТЕСТОВОЙ ВЫБОРКЕ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр alpha имеет очень важное значение: от его выбора зависит, как сильно мы будем штрафовать модель за переобучение. Важно найти значение, которое приносит наилучший эффект.\n",
    "\n",
    "Попробуйте вручную изменять параметр alpha для построенных ранее моделей. Согласитесь, это не очень удобно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём список из 20 возможных значений от 0.001 до 1\n",
    "alpha_list = np.linspace(0.001, 1, 20)\n",
    "# Создаём пустые списки, в которые будем добавлять результаты \n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for alpha in alpha_list:\n",
    "    # Создаём объект класса линейной регрессии с L1-регуляризацией\n",
    "    lasso_lr_poly = linear_model.Lasso(alpha=alpha, max_iter=10000)\n",
    "    \n",
    "    # Обучаем модель\n",
    "    lasso_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "    \n",
    "    # Делаем предсказание для тренировочной выборки\n",
    "    y_train_predict_poly = lasso_lr_poly.predict(X_train_scaled_poly)\n",
    "    # Делаем предсказание для тестовой выборки\n",
    "    y_test_predict_poly = lasso_lr_poly.predict(X_test_scaled_poly)\n",
    "    \n",
    "    # Рассчитываем коэффициенты детерминации для двух выборок и добавляем их в списки\n",
    "    train_scores.append(metrics.r2_score(y_train, y_train_predict_poly))\n",
    "    test_scores.append(metrics.r2_score(y_test, y_test_predict_poly))\n",
    "\n",
    "# Визуализируем изменение R^2 в зависимости от alpha\n",
    "fig, ax = plt.subplots(figsize=(12, 4)) # фигура + координатная плоскость\n",
    "ax.plot(alpha_list, train_scores, label='Train') # линейный график для тренировочной выборки\n",
    "ax.plot(alpha_list, test_scores, label='Test') # линейный график для тестовой выборки\n",
    "ax.set_xlabel('Alpha') # название оси абсцисс\n",
    "ax.set_ylabel('R^2') # название оси ординат\n",
    "ax.set_xticks(alpha_list) # метки по оси абсцисс\n",
    "ax.xaxis.set_tick_params(rotation=45) # поворот меток на оси абсцисс\n",
    "ax.legend(); # отображение легенды   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаём объект класса линейной регрессии с L1-регуляризацией\n",
    "lasso_lr_poly = linear_model.Lasso(alpha=0.0536)\n",
    "\n",
    "#Обучаем модель \n",
    "lasso_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "\n",
    "#Делаем предсказание для тренировочной выборки\n",
    "y_train_predict_poly = lasso_lr_poly.predict(X_train_scaled_poly)\n",
    "#Делаем предсказание для тестовой выборки\n",
    "y_test_predict_poly = lasso_lr_poly.predict(X_test_scaled_poly)\n",
    "\n",
    "#Рассчитываем коэффициент детерминации для двух выборок\n",
    "print(\"Train R^2: {:.3f}\".format(metrics.r2_score(y_train, y_train_predict_poly)))\n",
    "print(\"Test R^2: {:.3f}\".format(metrics.r2_score(y_test, y_test_predict_poly)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ В \"КЛАССИФИКАЦИИ\" С ПОМОЩЬЮ SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем объект класса логистическая регрессия\n",
    "log_reg_full = linear_model.LogisticRegression(random_state=42, max_iter=1000)\n",
    "# random_state — число, на основе которого происходит генерация случайных чисел.\n",
    "\n",
    "# multi_class='multinomial' - мультиклассовая классификация\n",
    "\n",
    "# penalty — метод регуляризации. Возможные значения:\n",
    "#   'l1' — L1-регуляризация;\n",
    "#   'l2' — L2-регуляризация (используется по умолчанию);\n",
    "#   'elasticnet' — эластичная сетка (L1+L2);\n",
    "#   'none' — отсутствие регуляризации.\n",
    "\n",
    "# C — коэффициент обратный коэффициенту регуляризации, то есть равен C/α. \n",
    "# Чем больше C, тем меньше регуляризация. По умолчанию C=1, тогда α=1.\n",
    "\n",
    "# solver — численный метод оптимизации функции потерь logloss, может быть:\n",
    "#   'sag' — стохастический градиентный спуск (нужна стандартизация/нормализация);\n",
    "#   'saga' — модификация предыдущего, которая поддерживает работу с негладкими функциями (нужна стандартизация/нормализация);\n",
    "#   'newton-cg' — метод Ньютона с модификацией сопряжённых градиентов (не нужна стандартизация/нормализация);\n",
    "#   'lbfgs' — метод Бройдена — Флетчера — Гольдфарба — Шанно (не нужна стандартизация/нормализация; используется по умолчанию, так как из всех методов теоретически обеспечивает наилучшую сходимость);\n",
    "#   'liblinear' — метод покоординатного спуска (не нужна стандартизация/нормализация).\n",
    "\n",
    "# max_iter — максимальное количество итераций, выделенных на сходимость.\n",
    "\n",
    "#Обучаем модель, минизируя logloss\n",
    "log_reg_full.fit(X, y)\n",
    "\n",
    "#Делаем предсказание класса\n",
    "y_pred = log_reg_full.predict(X)\n",
    "\n",
    "#Выводим результирующие коэффициенты\n",
    "print('w0: {}'.format(log_reg_full.intercept_)) #свободный член w0\n",
    "print('w1, w2: {}'.format(log_reg_full.coef_)) #остальные параметры модели w1, w2, ..., wm\n",
    "\n",
    "#Значения концентации глюкозы и индекса массы тела для пациента\n",
    "x_new = [[180, 51]]\n",
    "\n",
    "#Делаем предсказание класса:\n",
    "y_new_predict = log_reg_full.predict(x_new)\n",
    "print('Predicted class: {}'.format(y_new_predict))\n",
    "\n",
    "# Делаем предсказание вероятностей:\n",
    "y_new_proba_predict = log_reg_full.predict_proba(x_new)\n",
    "print('Predicted probabilities: {}'.format(np.round(y_new_proba_predict, 2)))\n",
    "\n",
    "#Создадим временную таблицу X\n",
    "X_temp = X.copy()\n",
    "#Добавим в эту таблицу результат предсказания\n",
    "X_temp['Prediction'] = y_pred\n",
    "X_temp.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЕРЕВЬЯ РЕШЕНИЙ В \"КЛАССИФИКАЦИИ\" С ПОМОЩЬЮ SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree #модели деревьев решения\n",
    "\n",
    "#Создаем объект класса дерево решений\n",
    "dt = tree.DecisionTreeClassifier(\n",
    "    criterion='entropy',\n",
    "    min_samples_leaf=5,\n",
    "    max_depth=8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# criterion — критерий информативности ('gini' — критерий Джини и 'entropy' — энтропия Шеннона).\n",
    "\n",
    "# max_depth — максимальная глубина дерева (по умолчанию — None, глубина дерева не ограничена).\n",
    "\n",
    "# max_features — максимальное число признаков, по которым ищется лучшее разбиение в дереве \n",
    "# (по умолчанию — None, то есть обучение производится на всех признаках). Нужно потому, \n",
    "# что при большом количестве признаков будет «дорого» искать лучшее (по критерию типа прироста информации) \n",
    "# разбиение среди всех признаков.\n",
    "\n",
    "# min_samples_leaf — минимальное число объектов в листе (по умолчанию — 1). \n",
    "# У этого параметра есть понятная интерпретация: если он равен 5, то дерево будет порождать \n",
    "# только те решающие правила, которые верны как минимум для пяти объектов.\n",
    "\n",
    "# random_state — число, отвечающее за генерацию случайных чисел.\n",
    "\n",
    "#Обучаем дерево по алгоритму CART\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "#Выводим значения метрики \n",
    "y_train_pred = dt.predict(X_train)\n",
    "print('Train: {:.2f}'.format(metrics.f1_score(y_train, y_train_pred)))\n",
    "y_test_pred = dt.predict(X_test)\n",
    "print('Test: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение дерева на графике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем фигуру для визуализации графа\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "#Строим граф дерева решений\n",
    "tree_graph = tree.plot_tree(\n",
    "    dt, #объект обученного дерева\n",
    "    feature_names=X_train.columns, #наименования факторов\n",
    "    class_names=[\"0 - <=50K\", \"1 - >50K\"], #имена классов\n",
    "    filled=True, #расцветка графа\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение графика с областями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probabilities_2d(X, y, model):\n",
    "    #Генерируем координатную сетку из всех возможных значений для признаков\n",
    "    #Glucose изменяется от 40 до 200, BMI — от 10 до 80\n",
    "    #Результат работы функции — два массива xx1 и xx2, которые образуют координатную сетку\n",
    "    xx1, xx2 = np.meshgrid(\n",
    "        np.arange(40, 200, 0.1),\n",
    "        np.arange(10, 80, 0.1)\n",
    "    )\n",
    "    #Вытягиваем каждый из массивов в вектор-столбец — reshape(-1, 1)\n",
    "    #Объединяем два столбца в таблицу с помощью hstack\n",
    "    X_net = np.hstack([xx1.reshape(-1, 1), xx2.reshape(-1, 1)])\n",
    "    #Предсказываем вероятность для всех точек на координатной сетке\n",
    "    #Нам нужна только вероятность класса 1\n",
    "    probs = model.predict_proba(X_net)[:, 1]\n",
    "    #Переводим столбец из вероятностей в размер координатной сетки\n",
    "    probs = probs.reshape(xx1.shape)\n",
    "    #Создаём фигуру и координатную плоскость\n",
    "    fig, ax = plt.subplots(figsize = (10, 5))\n",
    "    #Рисуем тепловую карту вероятностей\n",
    "    contour = ax.contourf(xx1, xx2, probs, 100, cmap='bwr')\n",
    "    #Рисуем разделяющую плоскость — линию, где вероятность равна 0.5\n",
    "    bound = ax.contour(xx1, xx2, probs, [0.5], linewidths=2, colors='black');\n",
    "    #Добавляем цветовую панель \n",
    "    colorbar = fig.colorbar(contour)\n",
    "    #Накладываем поверх тепловой карты диаграмму рассеяния\n",
    "    sns.scatterplot(data=X, x='Glucose', y='BMI', hue=y, palette='seismic', ax=ax)\n",
    "    #Даём графику название\n",
    "    ax.set_title('Scatter Plot with Decision Boundary');\n",
    "    #Смещаем легенду в верхний левый угол вне графика\n",
    "    ax.legend(bbox_to_anchor=(-0.05, 1))\n",
    "\n",
    "# Вызовем нашу функцию для визуализации:\n",
    "plot_probabilities_2d(X, y, dt_clf_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важность признаков можно посмотреть, обратившись к атрибуту feature_importance_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt_clf_full.feature_importances_)\n",
    "\n",
    "# А лучше через столбчатую диаграмму\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 5)) #фигура + координатная плоскость\n",
    "feature = X.columns #признаки\n",
    "feature_importances = dt_clf_full.feature_importances_ #важность признаков\n",
    "#Строим столбчатую диаграмму\n",
    "sns.barplot(x=feature, y=feature_importances, ax=ax);\n",
    "#Добавляем подпись графику, осям абсцисс и ординат\n",
    "ax.set_title('Bar plot feature importances')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_ylabel('Importances');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ансамбли"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ансамблевые модели или просто ансамбли (ensembles) — это метод машинного обучения, где несколько простых моделей (часто называемых «слабыми учениками») обучаются для решения одной и той же задачи и объединяются для получения лучших результатов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует три проверенных способа построения ансамблей:\n",
    "\n",
    "* Бэггинг — параллельно обучаем множество одинаковых моделей, а для предсказания берём среднее по предсказаниям каждой из моделей.\n",
    "* Бустинг — последовательно обучаем множество одинаковых моделей, где каждая новая модель концентрируется на тех примерах, где предыдущая допустила ошибку.\n",
    "* Стекинг — параллельно обучаем множество разных моделей, отправляем их результаты в финальную модель, и уже она принимает решение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем объект класса случайный лес\n",
    "rf = ensemble.RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=15,\n",
    "    criterion='entropy',\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42\n",
    ")\n",
    "#Обучаем модель\n",
    "rf.fit(X_train, y_train)\n",
    "#Выводим значения метрики \n",
    "y_train_pred = rf.predict(X_train)\n",
    "print('Train: {:.2f}'.format(metrics.f1_score(y_train, y_train_pred)))\n",
    "y_test_pred = rf.predict(X_test)\n",
    "print('Test: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
