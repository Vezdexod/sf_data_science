{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# СОДЕРЖАНИЕ\n",
    "\n",
    "Образец запроса с описательной информацией\n",
    "``` python\n",
    "from sklearn.mixture import GaussianMixture\n",
    "help(GaussianMixture)\n",
    "```\n",
    "\n",
    "## ***РАЗДЕЛЕНИЕ ВЫБОРОК И ВАЛИДАЦИЯ***\n",
    "* Деление на 3 набора:\n",
    "    * Тренировочный X_train, y_train (70-80%)\n",
    "    * Валидационный X_valid, y_valid (10-15%)\n",
    "    * Тестовый      X_test,  y_test  (10-15%)\n",
    "\n",
    "* ### HOLD-OUT - \"ОТЛОЖЕННАЯ ВЫБОРКА\" \n",
    "    * Очень простой и понятный\n",
    "    * Чаще всего применяется на больших датасетов, так как менне ресурсный\n",
    "    * Можно улучшить стратификацией `stratify=y`\n",
    "    ``` python\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    #разбиваем исходную выборку на тренировочную и валидационную в соотношении 80/20\n",
    "    X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X, y, test_size=0.2)\n",
    "    #разбиваем валидационную выборку на валидационную и тестовую в соотношении 50/50\n",
    "    X_valid, X_test, y_valid, y_test = model_selection.train_test_split(X_valid, y_valid, test_size=0.5)\n",
    "    ```\n",
    "\n",
    "* ### K-FOLD - \"КРОСС-ВАЛИДАЦИЯ ИЛИ ПЕРЕКРЁСТНЫЙ КОНТРОЛЬ\"\n",
    "    * Исходые данные разбивают на k-фолдов (частей) с отделением тестовых данных\n",
    "    * Циклично итерируют фолды, при этом один из них по очереди является валидационным\n",
    "    * Получаем более точный модели, менее чувствительные к выбросам\n",
    "    * Более ресурсный и медленный, так как число итераций зависит от числа фолдов\n",
    "    * Если количество фолдов больше 30, можно построить доверительный интервал для среднего значения метрики\n",
    "    ``` python\n",
    "    from sklearn import model_selection\n",
    "    kf = model_selection.KFold() # Обычное разбиение\n",
    "    skf = model_selection.StratifiedKFold() # Стратифицированное разбиение\n",
    "    #Считаем метрики на кросс-валидации k-fold\n",
    "    cv_metrics = model_selection.cross_validate()\n",
    "    print('Train k-fold mean accuracy: {:.2f}'.format(np.mean(cv_metrics['train_score'])))\n",
    "    ```\n",
    "\n",
    "* ### LIVE-ONE-OUT - \"ОТЛОЖЕННЫЙ ПРИМЕР ИЛИ ПОЭЛЕМЕНТНАЯ КРОСС-ВАЛИДАЦИЯ\"\n",
    "    * Алгоритм:\n",
    "        * Повторять n раз:\n",
    "            * Выбрать один случайный пример для валидации\n",
    "            * Обучить модель на всех оставшихся  примерах\n",
    "            * Произвести оценку качества (вычислить метрику) на отложенном примере\n",
    "        * Усреднить значение метрик на всех примерах\n",
    "    * Имеет наиболее объективные и надёжные метрики\n",
    "    * Идеален для небольших данных (порядка 100)\n",
    "    * Очень ресурсозатратный\n",
    "    ``` python\n",
    "    from sklearn import model_selection\n",
    "    loo = model_selection.LeaveOneOut()\n",
    "    cv_metrics = model_selection.cross_validate()\n",
    "    print('Train k-fold mean accuracy: {:.2f}'.format(np.mean(cv_metrics['train_score'])))\n",
    "    ```\n",
    "    * Примечание. Метод leave-one-out можно реализовать и без использования специального класса — достаточно просто указать параметр n_split=n в инициализаторе KFold, где n — количество строк в таблице.\n",
    "\n",
    "* ### БОРЬБА С ДИСБАЛАНСОМ\n",
    "    * Взвешивание объектов. В функцию ошибки добавляется штраф, прямо пропорциональный количеству объектов каждого класса. \n",
    "    * Выбор порога вероятности. Заключается в том, что мы подбираем такой порог вероятности (по умолчанию он равен 0.5 во всех моделях), при котором на валидационной выборке максимизируется целевая метрика.\n",
    "    * Сэмплирование (sampling) — перебалансировка выборки искусственным путём:  \n",
    "        * oversampling — искусственное увеличение количества объектов миноритарного класса;\n",
    "        * undersampling — сокращение количества объектов мажоритарного класса:\n",
    "            Здесь могут использоваться алгоритмы генерации искусственных данных, такие как NearMiss, SMOTE (Synthetic Minority Oversampling Techniques) и ADASYN (Adaptive Synthetic).\n",
    "            Мы рассмотрим наиболее популярный алгоритм — SMOTE\n",
    "        ``` python\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        sm = SMOTE(random_state=42)\n",
    "        X_train_s, y_train_s = sm.fit_resample(X_train, y_train)\n",
    "        ```\n",
    "---\n",
    "\n",
    "## ***ОТБОР И СЕЛЕКЦИЯ ПРИЗНАКОВ***\n",
    "* ### КОДИРОВАНИЕ ПРИЗНАКОВ\n",
    "    * Типы кодировки:\n",
    "        * Порядковое кодирование    `LabelEncoder`\n",
    "        * Двоичное кодирование      `LabelBinarizer`\n",
    "        * Однократное кодирование   `OneHotEncoder`\n",
    "    * Применение:\n",
    "        ``` python\n",
    "            # Импорт кодировщика LabelEncoder\n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "            # Инициализация кодировщика\n",
    "            encoder = LabelEncoder()\n",
    "            # Обучение (подгонака) кодировщика на выбранном признаке\n",
    "            encoder.fit(data['признак'])\n",
    "            # Преобразование выбранного признака\n",
    "            encoder.transform(data['признак'])\n",
    "\n",
    "            # Можно сразу обучать и преобразовывать\n",
    "            data_LabelEncoder = encoder.fit_transform(data['признак'])\n",
    "        ```\n",
    "    \n",
    "* ### РАБОТА С ПРОПУСКАМИ\n",
    "    * \n",
    "\n",
    "* ### РАБОТА С ВЫБРОСАМИ\n",
    "    * \n",
    "    ``` python\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    ```\n",
    "\n",
    "* ### МАСШТАБИРОВАНИЕ ПРИЗНАКОВ\n",
    "    * StandardScaler    `x_stand_scaled = (x - mean) / std_dev` (Для нормальных данных)\n",
    "    * MinMax    `x_minmax_scaled = (x - x_min) / (x_max - x_min)` (Не важна нормальность) (Можно менять диапазон)\n",
    "    * RobustScaler   `x_robust_scaled = (x - Q_1) / (Q_3 - Q_1)` (Не важна нормальность)\n",
    "\n",
    "* ### ТРАНСФОРМАЦИИ РАСПРЕДЕЛЕНИЙ ПРИЗНАКОВ\n",
    "    * #### QUANTILE TRANSFORMER\n",
    "        * `from sklearn.preprocessing import QuantileTransformer`\n",
    "    \n",
    "    * #### LOG TRANSFORM\n",
    "        * \n",
    "\n",
    "    * #### POWER TRANSFORMER SCALER\n",
    "        * `from sklearn.preprocessing import PowerTransformer`\n",
    "\n",
    "* ### ДАТЫ И РАССТОЯНИЯ\n",
    "    * #### ДАТЫ\n",
    "    \n",
    "    * #### РАССТОЯНИЯ\n",
    "\n",
    "    * #### ОТРИСОВКА ДАННЫХ НА КАРТЕ\n",
    "\n",
    "* ### ОТБОР ПРИЗНАКОВ: МОТИВАЦИЯ\n",
    "    * Это процесс выбора важных признаков, оказывающих наибольшее влияние на предсказание.\n",
    "    * Удалить один из мультиколлинеарных признаков, при этом оставить тот, который является \n",
    "        целым числом и легче обрабатывается\n",
    "    * Методы отбора признаков предназначены для уменьшения количества входных переменных до тех значений, \n",
    "        которые наиболее полезны для  предсказательной способности модели.\n",
    "\n",
    "* ### ОТБОР ПРИЗНАКОВ: КЛАССИФИКАЦИЯ МЕТОДОВ\n",
    "    * Метод рекурсивного исключения признаков (RFE) предполагает выбор признаков путём \n",
    "        рекурсивного рассмотрения всё меньших и меньших наборов фичей.   \n",
    "        * RFE - поиск хорошо работающих подмножеств фичей\n",
    "        ``` python\n",
    "            from sklearn.feature_selection import RFE\n",
    "            selector = RFE(estimator, n_features_to_select=3, step=1)\n",
    "            selector.ranking_\n",
    "            # array[[1, 1, 4, 1, 3, 2]] где 1 - самыеважные признаки, а 4 - менее важные\n",
    "        ```\n",
    "    * Фильтрация фичей на основе их связи с таргетом\n",
    "        ``` python\n",
    "            from sklearn.feature_selection import SelectKBest, f_regression\n",
    "        ```\n",
    "        * Библиотека sklearn обеспечивает реализацию большинства полезных статистических показателей, например:\n",
    "            * коэффициента корреляции Пирсона: f_regression()\n",
    "            * дисперсионного анализа ANOVA: f_classif()\n",
    "            * хи-квадрата: chi2()\n",
    "            * взаимной информации: mutual_info_classif() и mutual_info_regression()\n",
    "        * sklearn также предоставляет множество различных методов фильтрации после расчёта статистики для каждой входной переменной с целевой.\n",
    "            Два наиболее популярных метода:\n",
    "            * выбор k лучших переменных: SelectKBest\n",
    "            * выбор переменных верхнего процентиля: SelectPercentile\n",
    "            ``` python\n",
    "                from sklearn.feature_selection import SelectKBest, f_regression\n",
    "                selector = SelectKBest(f_regression, k=3)\n",
    "                selector.fit(X_train, y_train)                \n",
    "                selector.get_feature_names_out()\n",
    "            ```\n",
    "    * Алгоритмы, которые выполняют автоматический выбор фичей во время обучения (деревья)\n",
    "\n",
    "## ***ОПТИМИЗАЦИЯ ГИПЕРПАРАМЕТРОВ МОДЕЛИ***\n",
    "Параметры могут быть:\n",
    "* Внутренние (параметры модели):\n",
    "    * Подбираются во время обучения и определяют, как использовать входные данные для получения необходимого результата.\n",
    "    * Например, это веса (коэффициенты уравнения) в линейной/логистической регрессии.\n",
    "* Внешние (параметры алгоритма):\n",
    "    * Их принято называть гиперпараметрами. Внешние параметры могут быть произвольно установлены перед началом обучения и контролируют внутреннюю работу обучающего алгоритма.\n",
    "    * Гиперпараметры отвечают за сложность взаимосвязи между входными признаками и целевой переменной, поэтому сильно влияют на модель и качество прогнозирования.\n",
    "    * Например, это параметр регуляризации в линейной/логистической регрессии.\n",
    "\n",
    "* ### БАЗОВАЯ ОПТИМИЗАЦИЯ grid search\n",
    "    * \n",
    "    ``` python\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    ```\n",
    "\n",
    "* ### БАЗОВАЯ ОПТИМИЗАЦИЯ random search\n",
    "    * \n",
    "    ``` python\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    ```\n",
    "\n",
    "## ***РЕГРЕССИЯ***\n",
    "\n",
    "* ### АНАЛИТИЧЕСКОЕ РЕШЕНИЕ (NumPy)\n",
    "\n",
    "* ### АНАЛИТИЧЕСКОЕ РЕШЕНИЕ \"Линейная регрессия\" (sklearn)\n",
    "    ``` python\n",
    "    from sklearn import linear_model    \n",
    "    lr_model = linear_model.LinearRegression()\n",
    "    ```\n",
    "\n",
    "* ### ЧИСЛОВОЕ РЕШЕНИЕ \"Градиентный спуск (Gradient descent)\" (sklearn)\n",
    "    ``` python\n",
    "    from sklearn import linear_model\n",
    "    sgd_lr_model = linear_model.SGDRegressor()\n",
    "    ```\n",
    "\n",
    "* ### ПОЛИНОМИАЛЬНЫЕ ПРИЗНАКИ (Полиномиальная регрессия (Polynomial Regression))\n",
    "    * Нужно проводить стандартизацию и нормальзацию до этого этапа\n",
    "    ``` python\n",
    "    from sklearn import preprocessing\n",
    "    poly = preprocessing.PolynomialFeatures(degree=2, include_bias=False)\n",
    "    poly.fit(X_train)\n",
    "    X_train_poly = poly.transform(X_train)\n",
    "    ```\n",
    "    \n",
    "* ### РЕГУЛЯРИЗАЦИЯ (борьба с разбросом (переобучением)) \n",
    "    * L1-регуляризация (Lasso)\n",
    "        ``` python\n",
    "        lasso_lr_poly = linear_model.Lasso(alpha=0.1)\n",
    "        lasso_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "        ```\n",
    "    * L2-регуляризация (Ridge), или регуляризация Тихонова\n",
    "        ``` python\n",
    "        ridge_lr_poly = linear_model.Ridge(alpha=10)\n",
    "        ridge_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "        ```\n",
    "\n",
    "* ### МЕТРИКИ РЕГРЕССИИ \n",
    "    ``` python\n",
    "    from sklearn import metrics\n",
    "    ```\n",
    "    * MAE (Mean Absolute Error)             - mean_absolute_error(y, y_pred)\n",
    "    * MAPE (Mean Absolute Percent Error)    - mean_absolute_percentage_error(y, y_pred)*100\n",
    "    * MSE (Mean Square Error)               - mean_square_error(y, y_pred)\n",
    "    * RMSE (Root Mean Squared Error)        - np.sqrt(mean_square_error(y, y_pred))\n",
    "    * R2 (Коэффициент детерминации)         - r2_score(y, y_pred)\n",
    "\n",
    "---\n",
    "\n",
    "## ***КЛАССИФИКАЦИЯ***\n",
    "\n",
    "* ### ЧИСЛОВОЕ РЕШЕНИЕ \"Логистическая регрессия\"\n",
    "    ``` python\n",
    "    from sklearn import linear_model\n",
    "    log_reg_model = linear_model.LogisticRegression()\n",
    "    ```\n",
    "\n",
    "* ### МУЛЬТИКЛАССОВАЯ КЛАССИФИКАЦИЯ (когда не 2 класса, а больше)\n",
    "    * Сравнивает класс 0 с классом 1 и 2, потом класс 1 с классами 0 и 2 и т. д.\n",
    "    ``` python\n",
    "    from sklearn import linear_model\n",
    "    log_reg_model = linear_model.LogisticRegression(\n",
    "        multi_class='multinomial', #мультиклассовая классификация\n",
    "    )\n",
    "    ```\n",
    "\n",
    "* ### ДЕРЕВЬЯ РЕШЕНИЙ (связанный ациклический граф)\n",
    "    * состоит из корневой вершины, внутренних вершин и листьев\n",
    "    * Предикаты - критерии находящиеся на вершинах\n",
    "    ``` python\n",
    "    from sklearn import tree # Модели деревьев решения\n",
    "    # Создаём объект класса DecisionTreeClassifier\n",
    "    dt_clf_model = tree.DecisionTreeClassifier()\n",
    "    dt_clf_model.get_depth() # Показывает дерево\n",
    "    ```\n",
    "\n",
    "* ### АНСАМБЛИ (БЕГГИНГ) \"Случайный лес\"\n",
    "    * Много слабых моделей объединяются в одну\n",
    "    * Виды ансамблей \n",
    "        * Бэггинг — параллельно обучаем множество одинаковых моделей, а для предсказания берём среднее по предсказаниям каждой из моделей.\n",
    "        * Бустинг — последовательно обучаем множество одинаковых моделей, где каждая новая модель концентрируется на тех примерах, где предыдущая допустила ошибку.\n",
    "        * Стекинг — параллельно обучаем множество разных моделей, отправляем их результаты в финальную модель, и уже она принимает решение.\n",
    "    ``` python \n",
    "    from sklearn import ensemble\n",
    "    rf_clf_model = ensemble.RandomForestClassifier(\n",
    "        n_estimators=500, #число деревьев\n",
    "    )\n",
    "    ИЛИ\n",
    "    from  sklearn.ensemble import IsolationForest\n",
    "    ```\n",
    "\n",
    "* ### МЕТРИКИ КЛАССИФИКАЦИИ \n",
    "    ```python\n",
    "    from sklearn import metrics\n",
    "    ```\n",
    "    * Матрица ошибок                        - confusion_matrix(y, y_pred)\n",
    "    * Accuracy (достоверность)              - accuracy_score(y, y_pred)\n",
    "        * чем ближе к 1, тем больше угадала\n",
    "    * Precision (точность) или              - precision_score(y, y_pred)\n",
    "        * PPV (Positive Predictive Value)\n",
    "        * чем ближе к 1, тем меньше ошибок\n",
    "    * Recall (полнота) или                  - recall_score(y, y_pred)\n",
    "        * TPR (True Positive Rate)\n",
    "        * чем ближе к 1, тем больше значений 1 класса названо правильно\n",
    "    * F-мера                                - f1_score(y, y_pred)\n",
    "        * (это взвешенное среднее гармоническое между precision и recall)\n",
    "        * чем ближе к 1, тем точнее модель\n",
    "    * Все ошибки в одном отчете             - classification_report(y, y_pred)\n",
    "\n",
    "---\n",
    "\n",
    "## ***КЛАСТЕРИЗАЦИЯ***\n",
    "\n",
    "* ### ОПРЕДЕЛЕНИЕ ОПТИМАЛЬНОГО КОЛИЧЕСТВА КЛАСТЕРОВ\n",
    "    * #### МЕТОД ЛОКТЯ - построение графика зависимости инерции от количества кластеров\n",
    "        ``` python\n",
    "        model.inertia_ # получение инерции для создания графика\n",
    "        ```\n",
    "\n",
    "    * #### КОЭФФИЦИЕНТ СИЛУЭТА - построение графика зависимости коэффициента силуэта от количества кластеров\n",
    "        * Коэффициент силуэта показывает, насколько объект похож на объекты кластера, в котором он находится, по сравнению с объектами из других кластеров.\n",
    "        ``` python\n",
    "        silhouette_score(X, model.labels_)\n",
    "        ```\n",
    "* ### EM - АЛГОРИТМЫ КЛАСТЕРИЗАЦИИ\n",
    "    * В основе данного подхода лежит предположение, что любой объект принадлежит ко всем кластерам, но с разной вероятностью.\n",
    "    * Представителями являются:\n",
    "        * K-means кластеризация - например, для кластеризации документов\n",
    "        * GMM кластеризация - например, для сегментации изображения\n",
    "\n",
    "\n",
    "* ### КЛАСТЕРИЗАЦИЯ \"АЛГОРИТМ K-MEANS\"\n",
    "    * Чувствительна к выбросам\n",
    "    * K-MEANS - центроиды кластера это средние значения\n",
    "    * K-MEANS++ - центроиды кластера это средние значения, но первые значения выбираются не случайно\n",
    "    * K-MEDIANS - центроиды кластера это медианные значения\n",
    "    * K-MEDOIDS - центроиды кластера это медианные значения, но не расчетное значение, а ближайшая к нему точка\n",
    "    * FUZZY C-MEANS - каждый объект может принадлежать к разным кластерам с разной вероятностью\n",
    "    ``` python\n",
    "    from sklearn.cluster import KMeans\n",
    "    k_means_model = KMeans(n_clusters=2, init='k-means++', n_init=10, random_state=42)\n",
    "    ```\n",
    "* ### КЛАСТЕРИЗАЦИЯ \"GMM\" - модель гауссовой смеси (Gaussian Mixture Model)\n",
    "    * Чувствительна к выбросам\n",
    "    ``` python\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    gm_clst_model = GaussianMixture()\n",
    "    ```\n",
    "\n",
    "* ### \"ИЕРАРХИЧЕСКАЯ (АГЛОМЕРАТИВНАЯ)\" КЛАСТЕРИЗАЦИЯ\n",
    "    * Принцип иерархической кластеризации основан на построении дерева (иерархии) вложенных кластеров.\n",
    "    * Строится дендрограмма - это древовидная диаграмма, которая содержит  уровней. Каждый уровень — это шаг укрупнения кластеров.\n",
    "    * Методы построения дендрограммы:\n",
    "        * Агломеративный метод (agglomerative) - объединение мелких кластеров\n",
    "        * Дивизионный (дивизивный) метод (divisive) - деление крупных кластеров\n",
    "    * Методы расчета расстояния между кластерами\n",
    "        * Метод одиночной связи (min расстояние м/ж кластерами)_____`single`\n",
    "        * Метод полной связи____(max расстояние м/ж кластерами)____ `complete`\n",
    "        * Метод средней связи___(mean расстояние м/ж кластерами)___ `average`\n",
    "        * Центроидный метод_____(расстояние м/ж центрами кластеров)_`ward`\n",
    "    ``` python\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    agg_clst_model = AgglomerativeClustering()\n",
    "    ```\n",
    "\n",
    "* ### КЛАСТЕРИЗАЦИЯ НА ОСНОВЕПЛОТНОСТИ (DBSCAN)\n",
    "    * DENSITY-BASED SPATIAL CLUSTERING OF APPLICATIONS WITH NOISE\n",
    "    * В отличие от k-means, не нужно задавать количество кластеров — алгоритм сам определит оптимальное\n",
    "    * Алгоритм хорошо работает с данными произвольной формы\n",
    "    * DBSCAN отлично справляется с выбросами в датасетах\n",
    "    ``` python\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    clst_model = DBSCAN()    \n",
    "    ```\n",
    "    \n",
    "* ### **ПОНИЖЕНИЕ РАЗМЕРНОСТИ**\n",
    "    * #### СПЕКТРАЛЬНАЯ КЛАСТЕРИЗАЦИЯ\n",
    "        * \n",
    "        * Применяется для: \n",
    "            * сегментации изображений\n",
    "        ``` python\n",
    "        from sklearn.cluster import SpectralClustering\n",
    "        spectral_clst_model = SpectralClustering()\n",
    "        ```\n",
    "\n",
    "    * #### PCA - линейное преобразование\n",
    "        * Метод главных компонент, или PCA (Principal Components Analysis)\n",
    "        * это один из базовых способов уменьшения размерности\n",
    "        * Применяется для:  \n",
    "            * подавление шума\n",
    "            * индексация видео\n",
    "        ``` python\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA()\n",
    "        ```\n",
    "\n",
    "    * #### t-SNE - нелинейное преобразование\n",
    "        * t-SNE (t-distributed Stochastic Neighbor Embedding)\n",
    "        * «стохастическое вложение соседей с t-распределением»\n",
    "        * при преобразовании похожие объекты оказываются рядом, а непохожие — далеко друг от друга\n",
    "        * Применяется для:  \n",
    "            * уменьшения размерность до 2х или 3х мерного\n",
    "        ``` python\n",
    "        from sklearn.manifold import TSNE\n",
    "        tsne = TSNE()\n",
    "        ```\n",
    "\n",
    "\n",
    "* ### ВИЗУАЛИЗАЦИЯ КЛАСТЕРИЗАЦИЙ\n",
    "    * диаграмма рассеяния для двухмерного и трёхмерного случаев \n",
    "    * Convex Hull, или выпуклая оболочка - провести гарницы\n",
    "    * дендрограмма \n",
    "    * Clustergram - \"полосы\" - только для иерархических кластеризаций\n",
    "\n",
    "* ### МЕТРИКИ КЛАСТЕРИЗАЦИИ \n",
    "    ``` python\n",
    "    from sklearn.metrics.cluster import homogeneity_score\n",
    "    from sklearn.metrics.cluster import completeness_score\n",
    "    from sklearn.metrics.cluster import v_measure_score\n",
    "    from sklearn.metrics.cluster import rand_score\n",
    "    ```\n",
    "    * Однородность кластеров                - homogeneity_score(y, y_pred)\n",
    "    * Полнота кластеров                     - completeness_score(y, y_pred)\n",
    "    * V-мера (комбинация однор. и полн.)    - v_measure_score(y, y_pred)\n",
    "    * Индекс Ренда                          - rand_score(y, y_pred)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Базовые библиотеки\n",
    "import numpy as np # для матричных вычислений\n",
    "import pandas as pd # для анализа и предобработки данных\n",
    "import matplotlib.pyplot as plt # для визуализации\n",
    "import seaborn as sns # для визуализации\n",
    "\n",
    "# Модели\n",
    "from sklearn import linear_model # линейные модели\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn import tree # деревья\n",
    "from sklearn.cluster import KMeans # KMeans кластеризация\n",
    "from sklearn.mixture import GaussianMixture # GMM кластеризация\n",
    "from sklearn.cluster import SpectralClustering # Спектральная кластеризация\n",
    "from sklearn.decomposition import PCA # PCA снижение размерности\n",
    "from sklearn.manifold import TSNE # T-SNE снижение размерности\n",
    "from sklearn.cluster import DBSCAN # DBSCAN кластеризация\n",
    "from sklearn import ensemble # ансамбли\n",
    "from  sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Метрики регрессии и классификации\n",
    "from sklearn import metrics # метрики \n",
    "\n",
    "# Метрики кластеризации\n",
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "from sklearn.metrics.cluster import rand_score\n",
    "\n",
    "\n",
    "# Работа с данными для моделей\n",
    "from sklearn.model_selection import train_test_split # сплитование выборки\n",
    "from sklearn import model_selection # для K-Fold и LIVE-ONE-OUT\n",
    "\n",
    "\n",
    "#from imblearn.over_sampling import SMOTE # Для сэмплирования (придумывания) данных\n",
    "from sklearn import preprocessing # предобработка\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('pokemon.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# матрица наблюдений и вектор правильных ответов\n",
    "X, y = data.drop('RealClusters', axis=1), data['RealClusters'] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***РАЗДЕЛЕНИЕ ВЫБОРОК И ВАЛИДАЦИЯ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOLD-OUT - \"ОТЛОЖЕННАЯ ВЫБОРКА\"\n",
    "* Деление на 3 набора:\n",
    "    * Тренировочный X_train, y_train (70-80%)\n",
    "    * Валидационный X_valid, y_valid (10-15%)\n",
    "    * Тестовый      X_test,  y_test  (10-15%)\n",
    "* Очень простой и понятный\n",
    "* Чаще всего применяется на больших датасетов, так как менне ресурсный\n",
    "* Можно улучшить стратификацией `stratify=y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Разделяем выборку на тренировочную и валидационную в соотношении 70/30 со стратификацией\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Разделяем выборку на валидационную и тестовую в соотношении 50/50 со стратификацией\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=0.5, stratify=y_valid, random_state=42)\n",
    "\n",
    "# Выводим результирующие размеры таблиц\n",
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Valid:', X_valid.shape, y_valid.shape)\n",
    "print('Test:', X_test.shape, y_test.shape)\n",
    "\n",
    "# train_test_split(\n",
    "    # X - матрица признаков (наблюдений)\n",
    "    # y - вектор правильных ответов\n",
    "    # test_size=0.3 - 30% данных идут в тестовую выборку\n",
    "    # train_size - размер тренировочной выборки. Может быть указан в долях. \n",
    "        # Определяется автоматически, если параметр test_size передан как 1-test_size.    \n",
    "    # shuffle=False - По умолчанию True. Параметр перемешивания данных в выборке.\n",
    "    # stratify=y - Стратифицированное разбиение - Одинаковое соотношение целевого признака \n",
    "        # в тренировочной и тестовой выборке, не допускающее перекоса в обучении модели.\n",
    "    # random_state=42 - гарантирует выдачу генератором одних и тех же случайных значений\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-FOLD - \"КРОСС-ВАЛИДАЦИЯ ИЛИ ПЕРЕКРЁСТНЫЙ КОНТРОЛЬ\"\n",
    "* Исходые данные разбивают на k-фолдов (частей) с отделением тестовых данных\n",
    "* Циклично итерируют фолды, при этом один из них по очереди является валидационным\n",
    "* Получаем более точный модели, менее чувствительные к выбросам\n",
    "* Более ресурсный и медленный, так как число итераций зависит от числа фолдов\n",
    "* Если количество фолдов больше 30, можно построить доверительный интервал для среднего значения метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "# Создаём кросс-валидатор KFold\n",
    "kf = model_selection.KFold(n_splits=5) # Обычное разбиение\n",
    "skf = model_selection.StratifiedKFold(n_splits=5) # Стратифицированное разбиение\n",
    "\n",
    "# Создаём кросс-валидатор LeaveOneOut\n",
    "loo = model_selection.LeaveOneOut()\n",
    " \n",
    "#Считаем метрики на кросс-валидации k-fold\n",
    "cv_metrics = model_selection.cross_validate(\n",
    "    estimator=model, #модель\n",
    "    X=X, #матрица наблюдений X\n",
    "    y=y, #вектор ответов y\n",
    "    cv=kf, #кросс-валидатор KFold\n",
    "    #cv=skf, #кросс-валидатор SKFold\n",
    "    #cv=loo, #кросс-валидатор LeaveOneOut\n",
    "    scoring='accuracy', #метрика\n",
    "    return_train_score=True #подсчёт метрики на тренировочных фолдах\n",
    ")\n",
    "\n",
    "print('Train k-fold mean f1: {:.2f}'.format(np.mean(cv_metrics['train_score'])))\n",
    "print('Valid k-fold mean f1: {:.2f}'.format(np.mean(cv_metrics['test_score'])))\n",
    "\n",
    "#Делаем предсказание вероятностей на кросс-валидации\n",
    "y_cv_proba_pred = model_selection.cross_val_predict(model, X_train, y_train, cv=skf, method='predict_proba')\n",
    "\n",
    "#Выделяем столбец с вероятностями для класса 0\n",
    "y_cv_proba_pred_0 = y_cv_proba_pred[:, 0]\n",
    "\n",
    "#Выделяем столбец с вероятностями для класса 1 \n",
    "y_cv_proba_pred_1 = y_cv_proba_pred[:, 1]\n",
    "\n",
    "# KFold(\n",
    "    # n_splits=5 - число фолдов (частей)\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ВЫБОР ПОРОГА ВЕРОЯТНОСТИ. PR-КРИВАЯ.\n",
    "PR-кривая (precision-recall curve) — это график зависимости precision от recall при различных значениях порога вероятности.\n",
    "\n",
    "Мы можем построить PR-кривую. Для этого воспользуемся функций precision_recall_curve() из модуля metrics библиотеки sklearn. В данную функцию нужно передать истинные метки классов и предсказанные вероятности. Взамен она вернёт три массива: значения метрик precision и recall, вычисленных на различных порогах вероятности, и сами пороги вероятности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Вычисляем координаты PR-кривой\n",
    "precision, recall, thresholds = metrics.precision_recall_curve(y_train, y_cv_proba_pred)\n",
    "\n",
    "#Вычисляем F1-score при различных threshold\n",
    "f1_scores = (2 * precision * recall) / (precision + recall)\n",
    "#Определяем индекс максимума\n",
    "idx = np.argmax(f1_scores)\n",
    "print('Best threshold = {:.2f}, F1-Score = {:.2f}'.format(thresholds[idx], f1_scores[idx]))\n",
    " \n",
    "#Строим PR-кривую\n",
    "fig, ax = plt.subplots(figsize=(10, 5)) #фигура + координатная плоскость\n",
    "#Строим линейный график зависимости precision от recall\n",
    "ax.plot(precision, recall, label='Decision Tree PR')\n",
    "#Отмечаем точку максимума F1\n",
    "ax.scatter(precision[idx], recall[idx], marker='o', color='black', label='Best F1 score')\n",
    "#Даём графику название и подписываем оси\n",
    "ax.set_title('Precision-recall curve')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "#Отображаем легенду\n",
    "ax.legend();    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем предсказание классов с таким порогом для всех объектов из отложенной валидационной выборки и выведем отчёт о метриках:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нас интересует только вероятность класса (второй столбец)\n",
    "y_test_proba_pred = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "# Для удобства завернем numpy-массив в pandas Series\n",
    "y_test_proba_pred = pd.Series(y_test_proba_pred)\n",
    "# Создадим списки, в которых будем хранить значения метрик \n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "f1_scores = []\n",
    "# Сгенерируем набор вероятностных порогов в диапазоне от 0.1 до 1\n",
    "thresholds = np.arange(0.1, 1, 0.05)\n",
    "# В цикле будем перебирать сгенерированные пороги\n",
    "for threshold in thresholds:\n",
    "    # Пациентов, для которых вероятность наличия диабета > threshold относим к классу 1\n",
    "    # В противном случае - к классу 0\n",
    "    y_test_pred = y_test_proba_pred.apply(lambda x: 1 if x>threshold else 0)\n",
    "    # Считаем метрики и добавляем их в списки\n",
    "    recall_scores.append(metrics.recall_score(y_test, y_test_pred))\n",
    "    precision_scores.append(metrics.precision_score(y_test, y_test_pred))\n",
    "    f1_scores.append(metrics.f1_score(y_test, y_test_pred))\n",
    "\n",
    "# Визуализируем метрики при различных threshold\n",
    "fig, ax = plt.subplots(figsize=(10, 4)) #фигура + координатная плоскость\n",
    "# Строим линейный график зависимости recall от threshold\n",
    "ax.plot(thresholds, recall_scores, label='Recall')\n",
    "# Строим линейный график зависимости precision от threshold\n",
    "ax.plot(thresholds, precision_scores, label='Precision')\n",
    "\n",
    "# Строим линейный график зависимости F1 от threshold\n",
    "ax.plot(thresholds, f1_scores, label='F1-score')\n",
    "# Даем графику название и подписи осям\n",
    "ax.set_title('Recall/Precision dependence on the threshold')\n",
    "ax.set_xlabel('Probability threshold')\n",
    "ax.set_ylabel('Score')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задаем оптимальный порог вероятностей\n",
    "threshold_opt = 0.4 # полученное по графику значение\n",
    "#Образцы воды, для которых вероятность быть пригодными для питья > threshold_opt, относим к классу 1\n",
    "#В противном случае — к классу 0\n",
    "y_valid_pred_proba = model.predict_proba(X_valid)[:, 1]\n",
    "y_valid_pred = (y_valid_pred_proba > threshold_opt).astype('int')\n",
    "#Считаем метрики\n",
    "print(metrics.classification_report(y_valid, y_valid_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## СЭМПЛИРОВАНИЕ\n",
    "\n",
    "Следующий подход работы в условиях дисбаланса классов, который мы рассмотрим, — сэмплирование, а точнее — пересэмплирование (oversampling).\n",
    "\n",
    "Идея очень проста: если у нас мало наблюдений миноритарного класса, следует искусственно увеличить их количество."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для пользователей pip:\n",
    "\n",
    "`!pip install imbalanced-learn`\n",
    "Для пользователей anaconda:\n",
    "\n",
    "`!conda install -c conda-forge imbalanced-learn`\n",
    "Все алгоритмы пересэмплирования находятся в модуле over_sampling библиотеки imblearn. Импортируем оттуда алгоритм SMOTE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим объект класса SMOTE и вызовем у него метод fit_sample(), передав в него обучающую выборку (X_train, y_train). Затем выведем количество наблюдений каждого из классов до и после сэмплирования:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ТОЛЬКО ДЛЯ ТРЕНИРОВОЧНОЙ ВЫБОРКИ!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_s, y_train_s = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "print('Train shape before oversampling:', X_train.shape) \n",
    "print('Class balance before oversampling: \\n', y_train.value_counts(), sep='')\n",
    "print('-'*40)\n",
    "print('Train shape after oversampling:', X_train_s.shape)\n",
    "print('Class balance after oversampling: \\n', y_train_s.value_counts(), sep='')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## КРИВАЯ ОБУЧЕНИЯ (learning curve) \n",
    "* это график зависимости некоторой метрики на обучающем (валидационном) наборе данных от количества объектов, которые участвуют в обучении модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаём объект кросс-валидатора k-fold со стратификацией\n",
    "skf = model_selection.StratifiedKFold(n_splits=5)\n",
    " \n",
    "#Вычисляем координаты для построения кривой обучения\n",
    "train_sizes, train_scores, valid_scores = model_selection.learning_curve(\n",
    "    estimator = model, #модель\n",
    "    X = X, #матрица наблюдений X\n",
    "    y = y, #вектор ответов y\n",
    "    cv = skf, #кросс-валидатор\n",
    "    scoring = 'f1' #метрика\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(model, X, y, cv, scoring=\"f1\", ax=None, title=\"\"):\n",
    "    # Вычисляем координаты для построения кривой обучения\n",
    "    train_sizes, train_scores, valid_scores = model_selection.learning_curve(\n",
    "        estimator=model,  # модель\n",
    "        X=X,  # матрица наблюдений X\n",
    "        y=y,  # вектор ответов y\n",
    "        cv=cv,  # кросс-валидатор\n",
    "        scoring=scoring,  # метрика\n",
    "    )\n",
    "    # Вычисляем среднее значение по фолдам для каждого набора данных\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    valid_scores_mean = np.mean(valid_scores, axis=1)\n",
    "    # Если координатной плоскости не было передано, создаём новую\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))  # фигура + координатная плоскость\n",
    "    # Строим кривую обучения по метрикам на тренировочных фолдах\n",
    "    ax.plot(train_sizes, train_scores_mean, label=\"Train\")\n",
    "    # Строим кривую обучения по метрикам на валидационных фолдах\n",
    "    ax.plot(train_sizes, valid_scores_mean, label=\"Valid\")\n",
    "    # Даём название графику и подписи осям\n",
    "    ax.set_title(\"Learning curve: {}\".format(title))\n",
    "    ax.set_xlabel(\"Train data size\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    # Устанавливаем отметки по оси абсцисс\n",
    "    ax.xaxis.set_ticks(train_sizes)\n",
    "    # Устанавливаем диапазон оси ординат\n",
    "    ax.set_ylim(0, 1)\n",
    "    # Отображаем легенду\n",
    "    ax.legend()\n",
    "    \n",
    "# plot_learning_curve(\n",
    "    # estimator — модель, качество которой будет проверяться на кросс-валидации.\n",
    "    # X — матрица наблюдений.\n",
    "    # y — вектор-столбец правильных ответов.\n",
    "    # train_sizes — относительное (долевое) или абсолютное количество обучающих примеров, которые будут \n",
    "        # использоваться для создания кривой обучения. Если dtype имеет значение float, он \n",
    "        # рассматривается как часть максимального размера обучающего набора (который определяется выбранным методом проверки), \n",
    "        # т. е. он должен быть в пределах (0, 1].\n",
    "        # По умолчанию используется список [0.1, 0.325, 0.55, 0.775, 1.0], то есть для построения кривой обучения \n",
    "        # используется пять точек. Первая точка кривой обучения строится по 10 % наблюдений из обучающего набора, \n",
    "        # вторая точка — по 32.5 % и так далее до тех пор, пока в построении модели не будет участвовать весь обучающий набор данных.\n",
    "    # cv — кросс-валидатор из библиотеки sklearn (например, KFold) или количество фолдов, на которые необходимо разбить выборку. \n",
    "        # По умолчанию используется кросс-валидация k-fold на пяти фолдах.\n",
    "    # scoring — название метрики в виде строки либо функция для её вычисления.\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно сравниавть метрики разных моделей, загрузив их в список `models`.\n",
    "``` python\n",
    "    models = [\n",
    "        model_1,\n",
    "        ...,\n",
    "        model_n\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаём объект кросс-валидатора k-fold со стратификацией\n",
    "skf = model_selection.StratifiedKFold(n_splits=5)\n",
    "#Визуализируем кривые обучения\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4)) #фигура + три координатных плоскости\n",
    "#Создаём цикл по списку моделей и индексам этого списка\n",
    "for i, model in enumerate(models): #i — текущий индекс, model — текущая модель\n",
    "    plot_learning_curve(model, X, y, skf, ax=axes[i], title=f'model {i+1}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***РЕГРЕССИЯ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### АНАЛИТИЧЕСКОЕ РЕШЕНИЕ (NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y):\n",
    "    # Создаём вектор из единиц\n",
    "    ones = np.ones(X.shape[0])\n",
    "    # Добавляем вектор к таблице первым столбцом\n",
    "    X = np.column_stack([ones, X])\n",
    "    # Вычисляем обратную матрицу Q\n",
    "    Q = np.linalg.inv(X.T @ X)\n",
    "    # Вычисляем вектор коэффициентов\n",
    "    w = Q @ X.T @ y\n",
    "    return w\n",
    "# Вычисляем параметры линейной регрессии\n",
    "w = linear_regression(X, y)\n",
    "# Выводим вычисленные значения параметров в виде вектора\n",
    "print('Vector w: {}'.format(w))\n",
    "# Выводим параметры с точностью до двух знаков после запятой\n",
    "print('w0 = {:.2f}'.format(w[0]))\n",
    "print('w1 = {:.2f}'.format(w[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### АНАЛИТИЧЕСКОЕ РЕШЕНИЕ \"Линейная регрессия\" (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём объект класса LinearRegression\n",
    "lr_model = linear_model.LinearRegression()\n",
    "\n",
    "# Обучаем модель — ищем параметры по МНК\n",
    "lr_model.fit(X, y) \n",
    "\n",
    "# Выводим полученные параметры\n",
    "print('w0 = {}'.format(lr_model.intercept_)) #свободный член w0\n",
    "print('w1 = {}'.format(lr_model.coef_)) #остальные параметры модели w1, w2, ..., wm\n",
    "\n",
    "# Предсказываем медианную цену для всех участков из набора данных\n",
    "y_predict = lr_model.predict(X)\n",
    "\n",
    "# Выводим предсказание\n",
    "# Составляем таблицу из признаков и их коэффициентов\n",
    "w_df = pd.DataFrame({'Features': X.columns, 'Coefficients': lr_model.coef_})\n",
    "# Составляем строку таблицы со свободным членом\n",
    "intercept_df =pd.DataFrame({'Features': ['INTERCEPT'], 'Coefficients': lr_model.intercept_})\n",
    "coef_df = pd.concat([w_df, intercept_df], ignore_index=True)\n",
    "display(coef_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ЧИСЛОВОЕ РЕШЕНИЕ \"Градиентный спуск (Gradient descent)\" (sklearn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто требуется стандартизация данных, для коррекции масштаба. Предсказания на валидационных и тестовых выборках нужно делать так же после стандартизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    " \n",
    "# Инициализируем стандартизатор StandardScaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Подгоняем параметры стандартизатора (вычисляем среднее и СКО)\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Производим стандартизацию тренировочной выборки\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Производим стандартизацию тестовой выборки\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём объект класса линейной регрессии с SGD\n",
    "sgd_lr_model = linear_model.SGDRegressor(random_state=42)\n",
    "\n",
    "# Обучаем модель — ищем параметры по методу SGD\n",
    "sgd_lr_model.fit(X, y)\n",
    "\n",
    "# Выводим полученные параметры\n",
    "print('w0: {}'.format(sgd_lr_model.intercept_)) #свободный член w0\n",
    "print('w1: {}'.format(sgd_lr_model.coef_)) #остальные параметры модели w1, w2, ..., wm\n",
    "\n",
    "#Предсказываем медианную цену для всех участков из набора данных\n",
    "y_predict = sgd_lr_model.predict(X)\n",
    "\n",
    "# Выводим предсказание\n",
    "# Составляем таблицу из признаков и их коэффициентов\n",
    "w_df = pd.DataFrame({'Features': X.columns, 'Coefficients': sgd_lr_model.coef_})\n",
    "# Составляем строку таблицы со свободным членом\n",
    "intercept_df =pd.DataFrame({'Features': ['INTERCEPT'], 'Coefficients': sgd_lr_model.intercept_})\n",
    "coef_df = pd.concat([w_df, intercept_df], ignore_index=True)\n",
    "display(coef_df)\n",
    "\n",
    "# SGDRegressor(\n",
    "    # loss — функция потерь. По умолчанию используется squared_loss — уже привычная нам MSE. \n",
    "        # Но могут использоваться и несколько других. Например, значение \"huber\" определяет функцию потерь Хьюбера. \n",
    "        # Эта функция менее чувствительна к наличию выбросов, чем MSE.\n",
    "    # max_iter — максимальное количество итераций, выделенное на сходимость. Значение по умолчанию — 1000.\n",
    "    # learning_rate — режим управления темпом обучения. Значение по умолчанию — 'invscaling'. \n",
    "        # Этот режим уменьшает темп обучения по формуле, которую мы рассматривали ранее: etat=eta0/t^p.\n",
    "        # Есть ещё несколько режимов управления, о которых вы можете прочитать в документации.\n",
    "        # Если вы не хотите, чтобы темп обучения менялся на протяжении всего обучения, \n",
    "        # то можете выставить значение параметра на \"constant\"\n",
    "    # eta0 — начальное значение темпа обучения . Значение по умолчанию — 0.01\n",
    "        # Если параметр learning_rate=\"constant\", то значение этого параметра \n",
    "        # будет темпом обучения на протяжении всех итераций\n",
    "    # power_t — значение мощности уменьшения  в формуле etat=eta0/t^p . Значение по умолчанию — 0.25\n",
    "    # random_state=42 - гарантирует выдачу генератором одних и тех же случайных значений\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ПОЛИНОМИАЛЬНЫЕ ПРИЗНАКИ (Полиномиальная регрессия (Polynomial Regression))\n",
    "* Нужно проводить стандартизацию и нормальзацию до этого этапа\n",
    "``` python\n",
    "    from sklearn import preprocessing\n",
    "    poly = preprocessing.PolynomialFeatures(degree=2, include_bias=False)\n",
    "    poly.fit(X_train)\n",
    "    X_train_poly = poly.transform(X_train)\n",
    "```\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стандартизацию (нормализацию) полезнее проводить перед \n",
    "генерацией полиномиальных признаков, иначе можно потерять масштаб полиномов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    " \n",
    "# Инициализируем стандартизатор StandardScaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Подгоняем параметры стандартизатора (вычисляем среднее и СКО)\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Производим стандартизацию тренировочной выборки\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Производим стандартизацию тестовой выборки\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаём генератор полиномиальных признаков\n",
    "poly = preprocessing.PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly.fit(X_train_scaled)\n",
    "#Генерируем полиномиальные признаки для тренировочной выборки\n",
    "X_train_scaled_poly = poly.transform(X_train_scaled)\n",
    "#Генерируем полиномиальные признаки для тестовой выборки\n",
    "X_test_scaled_poly = poly.transform(X_test_scaled)\n",
    "#Выводим результирующие размерности таблиц\n",
    "print(X_train_scaled_poly.shape)\n",
    "print(X_test_scaled_poly.shape)\n",
    "\n",
    "# PolynomialFeatures(\n",
    "    # degree — степень полинома. По умолчанию используется степень 2\n",
    "    # include_bias — включать ли в результирующую таблицу столбец из единиц (x в степени 0). \n",
    "        # По умолчанию стоит True, но лучше выставить его в значение False, так как столбец из единиц и \n",
    "        # так добавляется в методе наименьших квадратов.\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### РЕГУЛЯРИЗАЦИЯ (борьба с разбросом (переобучением)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Недообучение (underfitting) — проблема, обратная переобучению. Модель из-за своей слабости не уловила никаких закономерностей в данных. В этом случае ошибка будет высокой как для тренировочных данных, так и для данных, не показанных во время обучения.\n",
    "\n",
    "Смещение (bias) — это математическое ожидание разности между истинным ответом и ответом, выданным моделью. То есть это ожидаемая ошибка модели.\n",
    "\n",
    "Разброс (variance) — это вариативность ошибки, то, насколько ошибка будет отличаться, если обучать модель на разных наборах данных. Математически это дисперсия (разброс) ответов модели.\n",
    "\n",
    "Регуляризация — способ уменьшения переобучения моделей машинного обучения.\n",
    "\n",
    "Штраф — это дополнительное неотрицательное слагаемое в выражении для функции потерь, которое специально повышает ошибку.  За счёт этого слагаемого метод оптимизации (OLS или SGD) будет находить не истинный минимум функции потерь, а псевдоминимум."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1-регуляризация (Lasso) \n",
    "# Создаём объект класса линейной регрессии с L1-регуляризацией\n",
    "lasso_lr_poly = linear_model.Lasso(alpha=0.1)\n",
    "\n",
    "# Обучаем модель\n",
    "lasso_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "\n",
    "# Делаем предсказание для тренировочной выборки\n",
    "y_train_predict_poly = lasso_lr_poly.predict(X_train_scaled_poly)\n",
    "# Делаем предсказание для тестовой выборки\n",
    "y_test_predict_poly = lasso_lr_poly.predict(X_test_scaled_poly)\n",
    "\n",
    "# Рассчитываем коэффициент детерминации для двух выборок\n",
    "print(\"Train R^2: {:.3f}\".format(metrics.r2_score(y_train, y_train_predict_poly)))\n",
    "print(\"Test R^2: {:.3f}\".format(metrics.r2_score(y_test, y_test_predict_poly)))\n",
    "\n",
    "# Lasso(\n",
    "    # Главный параметр инициализации Lasso — это alpha, коэффициент регуляризации. \n",
    "        # По умолчанию alpha=1. Практика показывает, что это довольно сильная регуляризация для L1-метода. \n",
    "        # Давайте установим значение этого параметра на 0.1.\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2-регуляризация (Ridge)\n",
    "# Создаём объект класса линейной регрессии с L2-регуляризацией\n",
    "ridge_lr_poly = linear_model.Ridge(alpha=10)\n",
    "\n",
    "# Обучаем модель\n",
    "ridge_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "\n",
    "# Делаем предсказание для тренировочной выборки\n",
    "y_train_predict_poly = ridge_lr_poly.predict(X_train_scaled_poly)\n",
    "# Делаем предсказание для тестовой выборки\n",
    "y_test_predict_poly = ridge_lr_poly.predict(X_test_scaled_poly)\n",
    "\n",
    "# Рассчитываем коэффициент детерминации для двух выборок\n",
    "print(\"Train R^2: {:.3f}\".format(metrics.r2_score(y_train, y_train_predict_poly)))\n",
    "print(\"Test R^2: {:.3f}\".format(metrics.r2_score(y_test, y_test_predict_poly)))\n",
    "\n",
    "# Ridge(\n",
    "    # Главный параметр инициализации Lasso — это alpha, коэффициент регуляризации. \n",
    "        # Для L2-регуляризации параметр alpha по умолчанию равен 1. \n",
    "        # Давайте попробуем использовать значение параметра alpha=10:\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ЛУЧШЕ ТА, У КОТОРОЙ ВЫШЕ ПОКАЗАТЕЛИ НА ТЕСТОВОЙ ВЫБОРКЕ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр alpha имеет очень важное значение: от его выбора зависит, как сильно мы будем штрафовать модель за переобучение. Важно найти значение, которое приносит наилучший эффект.\n",
    "\n",
    "Попробуйте вручную изменять параметр alpha для построенных ранее моделей. Согласитесь, это не очень удобно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём список из 20 возможных значений от 0.001 до 1\n",
    "alpha_list = np.linspace(0.001, 1, 20)\n",
    "# Создаём пустые списки, в которые будем добавлять результаты \n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for alpha in alpha_list:\n",
    "    # Создаём объект класса линейной регрессии с L1-регуляризацией\n",
    "    lasso_lr_poly = linear_model.Lasso(alpha=alpha, max_iter=10000)\n",
    "    \n",
    "    # Обучаем модель\n",
    "    lasso_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "    \n",
    "    # Делаем предсказание для тренировочной выборки\n",
    "    y_train_predict_poly = lasso_lr_poly.predict(X_train_scaled_poly)\n",
    "    # Делаем предсказание для тестовой выборки\n",
    "    y_test_predict_poly = lasso_lr_poly.predict(X_test_scaled_poly)\n",
    "    \n",
    "    # Рассчитываем коэффициенты детерминации для двух выборок и добавляем их в списки\n",
    "    train_scores.append(metrics.r2_score(y_train, y_train_predict_poly))\n",
    "    test_scores.append(metrics.r2_score(y_test, y_test_predict_poly))\n",
    "\n",
    "# Визуализируем изменение R^2 в зависимости от alpha\n",
    "fig, ax = plt.subplots(figsize=(12, 4)) # фигура + координатная плоскость\n",
    "ax.plot(alpha_list, train_scores, label='Train') # линейный график для тренировочной выборки\n",
    "ax.plot(alpha_list, test_scores, label='Test') # линейный график для тестовой выборки\n",
    "ax.set_xlabel('Alpha') # название оси абсцисс\n",
    "ax.set_ylabel('R^2') # название оси ординат\n",
    "ax.set_xticks(alpha_list) # метки по оси абсцисс\n",
    "ax.xaxis.set_tick_params(rotation=45) # поворот меток на оси абсцисс\n",
    "ax.legend(); # отображение легенды   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбираем на графике лучший результат *alpha*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём объект класса линейной регрессии с L1-регуляризацией\n",
    "lasso_lr_poly = linear_model.Lasso(alpha=0.0536)\n",
    "\n",
    "# Обучаем модель \n",
    "lasso_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "\n",
    "# Делаем предсказание для тренировочной выборки\n",
    "y_train_predict_poly = lasso_lr_poly.predict(X_train_scaled_poly)\n",
    "# Делаем предсказание для тестовой выборки\n",
    "y_test_predict_poly = lasso_lr_poly.predict(X_test_scaled_poly)\n",
    "\n",
    "# Рассчитываем коэффициент детерминации для двух выборок\n",
    "print(\"Train R^2: {:.3f}\".format(metrics.r2_score(y_train, y_train_predict_poly)))\n",
    "print(\"Test R^2: {:.3f}\".format(metrics.r2_score(y_test, y_test_predict_poly)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### МЕТРИКИ РЕГРЕССИИ \n",
    "``` python\n",
    "    from sklearn import metrics\n",
    "```\n",
    "* MAE (Mean Absolute Error)             - `mean_absolute_error(y, y_pred)` (чем меньше, тем лучше)\n",
    "* MAPE (Mean Absolute Percent Error)    - `mean_absolute_percentage_error(y, y_pred)*100` (чем меньше, тем лучше)\n",
    "* MSE (Mean Square Error)               - `mean_square_error(y, y_pred)` (чем меньше, тем лучше)\n",
    "* RMSE (Root Mean Squared Error)        - `np.sqrt(mean_square_error(y, y_pred))` (чем меньше, тем лучше)\n",
    "* R2 (Коэффициент детерминации)         - `r2_score(y, y_pred)` (чем больше, тем лучше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Делаем предсказание по всем признакам\n",
    "y_predict = model.predict(X)\n",
    "# Рассчитываем MAE - Средняя абсолютная ошибка\n",
    "print('MAE score: {:.3f}'.format(metrics.mean_absolute_error(y_test, y_predict)))\n",
    "# Рассчитываем MAPE - Средняя абсолютная ошибка в процентах\n",
    "print('MAPE score: {:.3f} %'.format(metrics.mean_absolute_percentage_error(y_test, y_predict) * 100))\n",
    "# Рассчитываем RMSE - Корень из средней квадратической ошибки\n",
    "print('RMSE score: {:.3f}'.format(np.sqrt(metrics.mean_squared_error(y_test, y_predict))))\n",
    "# Рассчитываем коэффициент детерминации Коэффициент детерминации (R2)\n",
    "print('R2 score: {:.3f}'.format(metrics.r2_score(y_test, y_predict)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***КЛАССИФИКАЦИЯ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ЧИСЛОВОЕ РЕШЕНИЕ \"Логистическая регрессия\"\n",
    "* Основано на методе максимального правдоподобия\n",
    "``` python\n",
    "    from sklearn import linear_model\n",
    "    log_reg_model = linear_model.LogisticRegression()\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### МУЛЬТИКЛАССОВАЯ КЛАССИФИКАЦИЯ (когда не 2 класса, а больше)\n",
    "* Сравнивает класс 0 с классом 1 и 2, потом класс 1 с классами 0 и 2 и т. д.\n",
    "``` python\n",
    "    from sklearn import linear_model\n",
    "    log_reg_model = linear_model.LogisticRegression(\n",
    "        multi_class='multinomial', #мультиклассовая классификация\n",
    "    )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем объект класса логистическая регрессия\n",
    "log_reg_model = linear_model.LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Обучаем модель, минизируя logloss\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Делаем предсказание класса\n",
    "y_pred = log_reg_model.predict(X_train)\n",
    "\n",
    "# Делаем предсказание вероятностей:\n",
    "y_new_proba_predict = log_reg_model.predict_proba(X_train)\n",
    "\n",
    "# Выводим результирующие коэффициенты\n",
    "print('w0: {}'.format(log_reg_model.intercept_)) #свободный член w0\n",
    "print('w1, w2: {}'.format(log_reg_model.coef_)) #остальные параметры модели w1, w2, ..., wm\n",
    "\n",
    "\n",
    "# LogisticRegression(\n",
    "    # multi_class='multinomial' - мультиклассовая классификация\n",
    "    # penalty — метод регуляризации. Возможные значения:\n",
    "        # 'l1' — L1-регуляризация\n",
    "        # 'l2' — L2-регуляризация (используется по умолчанию)\n",
    "        # 'elasticnet' — эластичная сетка (L1+L2)\n",
    "        # 'none' — отсутствие регуляризации\n",
    "    # C — коэффициент обратный коэффициенту регуляризации, то есть равен C/α\n",
    "        # Чем больше C, тем меньше регуляризация. По умолчанию C=1, тогда α=1\n",
    "    # solver — численный метод оптимизации функции потерь logloss, может быть:\n",
    "        # 'sag' — стохастический градиентный спуск (нужна стандартизация/нормализация)\n",
    "        # 'saga' — модификация предыдущего, которая поддерживает работу с негладкими функциями (нужна стандартизация/нормализация)\n",
    "        # 'newton-cg' — метод Ньютона с модификацией сопряжённых градиентов (не нужна стандартизация/нормализация)\n",
    "        # 'lbfgs' — метод Бройдена — Флетчера — Гольдфарба — Шанно (не нужна стандартизация/нормализация; используется по умолчанию, \n",
    "            # так как из всех методов теоретически обеспечивает наилучшую сходимость)\n",
    "        # 'liblinear' — метод покоординатного спуска (не нужна стандартизация/нормализация)\n",
    "    # max_iter — максимальное количество итераций, выделенных на сходимость\n",
    "    # class_weight='balanced' - сбалансированность весов классов для дисбалансных классификаций\n",
    "    # random_state=42 - гарантирует выдачу генератором одних и тех же случайных значений  \n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ДЕРЕВЬЯ РЕШЕНИЙ (связанный ациклический граф)\n",
    "* состоит из корневой вершины, внутренних вершин и листьев\n",
    "* Предикаты - критерии находящиеся на вершинах\n",
    "``` python\n",
    "    from sklearn import tree # Модели деревьев решения\n",
    "    # Создаём объект класса DecisionTreeClassifier\n",
    "    dt_clf_model = tree.DecisionTreeClassifier()\n",
    "    dt_clf_model.get_depth() # Показывает дерево\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree # модели деревьев решения\n",
    "\n",
    "# Создаем объект класса дерево решений\n",
    "dt_model = tree.DecisionTreeClassifier(\n",
    "    criterion='entropy',\n",
    "    min_samples_leaf=5,\n",
    "    max_depth=8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Обучаем дерево по алгоритму CART\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Выводим значения метрики \n",
    "y_train_pred = dt_model.predict(X_train)\n",
    "print('Train: {:.2f}'.format(metrics.f1_score(y_train, y_train_pred)))\n",
    "y_test_pred = dt_model.predict(X_test)\n",
    "print('Test: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))\n",
    "\n",
    "# DecisionTreeClassifier(\n",
    "    # criterion — критерий информативности ('gini' — критерий Джини и 'entropy' — энтропия Шеннона).\n",
    "    # max_depth — максимальная глубина дерева (по умолчанию — None, глубина дерева не ограничена).\n",
    "    # max_features — максимальное число признаков, по которым ищется лучшее разбиение в дереве \n",
    "        # (по умолчанию — None, то есть обучение производится на всех признаках). Нужно потому, \n",
    "        # что при большом количестве признаков будет «дорого» искать лучшее (по критерию типа прироста информации) \n",
    "        # разбиение среди всех признаков.\n",
    "    # min_samples_leaf — минимальное число объектов в листе (по умолчанию — 1). \n",
    "        # У этого параметра есть понятная интерпретация: если он равен 5, то дерево будет порождать \n",
    "        # только те решающие правила, которые верны как минимум для пяти объектов.\n",
    "    # random_state=42 - гарантирует выдачу генератором одних и тех же случайных значений\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение дерева на графике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем фигуру для визуализации графа\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "# Строим граф дерева решений\n",
    "tree_graph = tree.plot_tree(\n",
    "    dt_model, # объект обученного дерева\n",
    "    feature_names=X_train.columns, # наименования факторов\n",
    "    class_names=[\"0 - <=50K\", \"1 - >50K\"], # имена классов\n",
    "    filled=True, # расцветка графа\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение графика с областями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probabilities_2d(X, y, model):\n",
    "    # Генерируем координатную сетку из всех возможных значений для признаков\n",
    "    # Glucose изменяется от 40 до 200, BMI — от 10 до 80\n",
    "    # Результат работы функции — два массива xx1 и xx2, которые образуют координатную сетку\n",
    "    xx1, xx2 = np.meshgrid(\n",
    "        np.arange(40, 200, 0.1),\n",
    "        np.arange(10, 80, 0.1)\n",
    "    )\n",
    "    # Вытягиваем каждый из массивов в вектор-столбец — reshape(-1, 1)\n",
    "    # Объединяем два столбца в таблицу с помощью hstack\n",
    "    X_net = np.hstack([xx1.reshape(-1, 1), xx2.reshape(-1, 1)])\n",
    "    # Предсказываем вероятность для всех точек на координатной сетке\n",
    "    # Нам нужна только вероятность класса 1\n",
    "    probs = model.predict_proba(X_net)[:, 1]\n",
    "    # Переводим столбец из вероятностей в размер координатной сетки\n",
    "    probs = probs.reshape(xx1.shape)\n",
    "    # Создаём фигуру и координатную плоскость\n",
    "    fig, ax = plt.subplots(figsize = (10, 5))\n",
    "    # Рисуем тепловую карту вероятностей\n",
    "    contour = ax.contourf(xx1, xx2, probs, 100, cmap='bwr')\n",
    "    # Рисуем разделяющую плоскость — линию, где вероятность равна 0.5\n",
    "    bound = ax.contour(xx1, xx2, probs, [0.5], linewidths=2, colors='black');\n",
    "    # Добавляем цветовую панель \n",
    "    colorbar = fig.colorbar(contour)\n",
    "    # Накладываем поверх тепловой карты диаграмму рассеяния\n",
    "    sns.scatterplot(data=X, x='Glucose', y='BMI', hue=y, palette='seismic', ax=ax)\n",
    "    # Даём графику название\n",
    "    ax.set_title('Scatter Plot with Decision Boundary');\n",
    "    # Смещаем легенду в верхний левый угол вне графика\n",
    "    ax.legend(bbox_to_anchor=(-0.05, 1))\n",
    "\n",
    "# Вызовем нашу функцию для визуализации:\n",
    "plot_probabilities_2d(X, y, dt_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важность признаков можно посмотреть, обратившись к атрибуту feature_importance_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt_model.feature_importances_)\n",
    "\n",
    "# А лучше через столбчатую диаграмму\n",
    "fig, ax = plt.subplots(figsize=(13, 5)) #фигура + координатная плоскость\n",
    "feature = X.columns #признаки\n",
    "feature_importances = dt_model.feature_importances_ #важность признаков\n",
    "# Строим столбчатую диаграмму\n",
    "sns.barplot(x=feature, y=feature_importances, ax=ax);\n",
    "# Добавляем подпись графику, осям абсцисс и ординат\n",
    "ax.set_title('Bar plot feature importances')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_ylabel('Importances');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### АНСАМБЛИ (БЕГГИНГ) \"Случайный лес\"\n",
    "* Много слабых моделей объединяются в одну\n",
    "* Виды ансамблей \n",
    "    * Бэггинг — параллельно обучаем множество одинаковых моделей, а для предсказания берём среднее по предсказаниям каждой из моделей.\n",
    "    * Бустинг — последовательно обучаем множество одинаковых моделей, где каждая новая модель концентрируется на тех примерах, где предыдущая допустила ошибку.\n",
    "    * Стекинг — параллельно обучаем множество разных моделей, отправляем их результаты в финальную модель, и уже она принимает решение.\n",
    "``` python \n",
    "    from sklearn import ensemble\n",
    "    rf_clf_model = ensemble.RandomForestClassifier(\n",
    "        n_estimators=500, #число деревьев\n",
    "    )\n",
    "```\n",
    "* Случайный лес (Random Forest)** - частный случай бэггинга над деревьями решений с методом случайных подпространств"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём объект класса RandomForestClassifier\n",
    "rf_model = ensemble.RandomForestClassifier(\n",
    "    n_estimators=500, # число деревьев\n",
    "    criterion='entropy', # критерий эффективности\n",
    "    max_depth=3, # максимальная глубина дерева\n",
    "    min_samples_leaf=10, # минимальное число объектов в листе\n",
    "    max_features='sqrt', # число признаков из метода случайных подпространств\n",
    "    random_state=42 # генератор случайных чисел\n",
    ")\n",
    "\n",
    "# Обучаем модель \n",
    "rf_model.fit(X_train, y_train)\n",
    " \n",
    "#Выводим значения метрики \n",
    "y_train_pred = rf_model.predict(X_train)\n",
    "print('Train: {:.2f}'.format(metrics.f1_score(y_train, y_train_pred)))\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "print('Test: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))\n",
    "\n",
    "# RandomForestClassifier(\n",
    "    # n_estimators — количество деревьев в лесу (число K из бэггинга; по умолчанию равно 100)\n",
    "    # criterion — критерий информативности разбиения для каждого из деревьев \n",
    "        # ('gini' — критерий Джини и 'entropy' — энтропия Шеннона; по умолчанию — 'gini')\n",
    "    # max_depth — максимальная глубина одного дерева (по умолчанию — None, то есть глубина дерева не ограничена)\n",
    "    # max_features — максимальное число признаков, которые будут использоваться каждым из деревьев \n",
    "        # (число L из метода случайных подпространств; по умолчанию — 'sqrt';\n",
    "        # для обучения каждого из деревьев используется корень из m признаков, \n",
    "        # где m — число признаков в начальном наборе данных)\n",
    "    # min_samples_leaf — минимальное число объектов в листе (по умолчанию — 1)\n",
    "    # random_state=42 - гарантирует выдачу генератором одних и тех же случайных значений\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вызовем нашу функцию для визуализации описанную в деревьях решений выше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вызовем нашу функцию для визуализации:\n",
    "plot_probabilities_2d(X, y, rf_model)\n",
    "\n",
    "# Также можно показать важность признаков через столбчатую диаграмму"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_proba_pred = pd.Series(rf_model.predict_proba(X_test)[:, 1])\n",
    "#Создадим списки, в которых будем хранить значения метрик \n",
    "f1_scores = []\n",
    "#Сгенерируем набор вероятностных порогов в диапазоне от 0.1 до 1\n",
    "thresholds = np.arange(0.1, 1, 0.05)\n",
    "#В цикле будем перебирать сгенерированные пороги\n",
    "for threshold in thresholds:\n",
    "    y_test_pred_poly = y_test_proba_pred.apply(lambda x: 1 if x > threshold else 0)\n",
    "    #Считаем метрики и добавляем их в списки\n",
    "    f1_scores.append(metrics.f1_score(y_test, y_test_pred_poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Визуализируем метрики при различных threshold\n",
    "fig, ax = plt.subplots(figsize=(10, 4)) #фигура + координатная плоскость\n",
    "#Строим линейный график зависимости F1 от threshold\n",
    "ax.plot(thresholds, f1_scores, label='F1-score')\n",
    "\n",
    "#Даем графику название и подписи осям\n",
    "ax.set_title('F1 dependence on the threshold')\n",
    "ax.set_xlabel('Probability threshold')\n",
    "ax.set_ylabel('Score')\n",
    "#Устанавливаем отметки по оси x\n",
    "ax.set_xticks(thresholds) \n",
    "# Подписываем названия осей\n",
    "ax.set_xlabel('Probability threshold')\n",
    "ax.set_ylabel('Score')\n",
    "#Отображаем легенду\n",
    "ax.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### МЕТРИКИ КЛАССИФИКАЦИИ \n",
    "```python\n",
    "    from sklearn import metrics\n",
    "```\n",
    "* Матрица ошибок                        - `confusion_matrix(y, y_pred)`\n",
    "* Accuracy (достоверность)              - `accuracy_score(y, y_pred)`\n",
    "    * чем ближе к 1, тем больше угадала\n",
    "    * бесполезна при дисбалансе\n",
    "* Precision (точность) или              - `precision_score(y, y_pred)`\n",
    "    * PPV (Positive Predictive Value)\n",
    "    * чем ближе к 1, тем меньше ошибок\n",
    "    * нужна там, где должно быть минимум ложных срабатываний\n",
    "    * можно использовать при дисбалансе\n",
    "* Recall (полнота) или                  - `recall_score(y, y_pred)`\n",
    "    * TPR (True Positive Rate)\n",
    "    * чем ближе к 1, тем больше значений 1 класса названо правильно\n",
    "    * можно использовать при дисбалансе\n",
    "* F-мера                                - `f1_score(y, y_pred)`\n",
    "    * (это взвешенное среднее гармоническое между precision и recall)\n",
    "    * чем ближе к 1, тем точнее модель\n",
    "    * Используется в задачах, где необходимо балансировать между precision и recall\n",
    "* Все ошибки в одном отчете             - `classification_report(y, y_pred)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Матрица ошибок\n",
    "print('Matrix_error: {}'.format(metrics.confusion_matrix(y, y_pred)))\n",
    "\n",
    "#Рассчитываем accuracy\n",
    "print('Accuracy: {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
    "#Рассчитываем precision\n",
    "print('Precision: {:.2f}'.format(metrics.precision_score(y, y_pred)))\n",
    "#Рассчитываем recall\n",
    "print('Recall: {:.2f}'.format(metrics.recall_score(y, y_pred)))\n",
    "#Рассчитываем F1-меру\n",
    "print('F1 score: {:.2f}'.format(metrics.f1_score(y, y_pred)))\n",
    "\n",
    "# Метрики классификации в одной строке\n",
    "print(metrics.classification_report(y, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что отображено в print(metrics.classification_report(y, y_pred))?\n",
    "\n",
    "1) В первой части таблицы отображаются метрики precision, recall и f1-score, рассчитанные для каждого класса в отдельности. Столбец support — это количество объектов каждого из классов.\n",
    "2) Во второй части таблицы отображена общая метрика accuracy. \n",
    "3) Далее идёт строка macro avg — это среднее значение метрики между классами 1 и 0. Например, значение в строке macro avg и столбце recall = (0.88 + 0.56)/2=0.72.\n",
    "4) Завершает отчёт строка weighted avg — это средневзвешенное значение метрики между классами 1 и 0. Рассчитывается по формуле:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***КЛАСТЕРИЗАЦИЯ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ОПРЕДЕЛЕНИЕ ОПТИМАЛЬНОГО КОЛИЧЕСТВА КЛАСТЕРОВ\n",
    "* #### МЕТОД ЛОКТЯ - построение графика зависимости инерции от количества кластеров\n",
    "    ``` python\n",
    "        model.inertia_ # получение инерции для создания графика\n",
    "    ```\n",
    "\n",
    "* #### КОЭФФИЦИЕНТ СИЛУЭТА - построение графика зависимости коэффициента силуэта от количества кластеров\n",
    "    * Коэффициент силуэта показывает, насколько объект похож на объекты кластера, в котором он находится, по сравнению с объектами из других кластеров.\n",
    "    ``` python\n",
    "    silhouette_score(X, model.labels_)\n",
    "    ```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### МЕТОД ЛОКТЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# функция, которая принимает количество кластеров для k-means и матрицу с признаками объектов,и возвращает инерцию \n",
    "def get_inertia(cluster_num, X):\n",
    "# инициализируем алгоритм кластеризации\n",
    "    k_means =  KMeans(n_clusters=cluster_num, random_state=42)\n",
    "# запускаем алгоритм k-means\n",
    "    k_means.fit(X)\n",
    "# находим значение инерции\n",
    "    inertia = k_means.inertia_\n",
    "# возвращаем значение инерции\n",
    "    return inertia\n",
    "\n",
    "# создадим пустой словарь, ключами будут инерция и количество кластеров\n",
    "res = {\"inertia\": [], \"cluster\": []}\n",
    "\n",
    "# выберем нужные данные \n",
    "X = df[['Attack', 'Defense']]\n",
    "\n",
    "# итерируемся по разным размерам кластеров (от 1 до 9) и сохраним значение инерции для каждого кластера\n",
    "for cluster_num in range(1, 10):\n",
    "# сохраняем значения\n",
    "    res[\"inertia\"].append(get_inertia(cluster_num, X))\n",
    "    res[\"cluster\"].append(cluster_num)\n",
    "\n",
    "# сохраним в датафрейм значение инерции и количество кластеров\n",
    "res_df = pd.DataFrame(res)\n",
    "\n",
    "# установим стиль для визуализиции\n",
    "sns.set_style(\"darkgrid\")\n",
    "# визуализируем зависимость значения инерции от количества кластеров\n",
    "sns.lineplot(data=res_df, x=\"cluster\", y=\"inertia\", marker= \"o\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### КОЭФФИЦИЕНТ СИЛУЭТА"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем метрику силуэт\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# напишем функцию, как и при подсчете метода локтя\n",
    "def get_silhouette(cluster_num, X):\n",
    "    k_means =  KMeans(n_clusters=cluster_num, init='k-means++', n_init=10, random_state=42)\n",
    "    k_means.fit(X)\n",
    "# подсчитаем метрику силуэта, передав данные и то, к каким кластерам относятся объекты\n",
    "    silhouette = silhouette_score(X, k_means.predict(X))\n",
    "    return silhouette\n",
    "\n",
    "# создадим пустой словарь, ключами будут инерция и количество кластеров\n",
    "silhouette_res = {\"silhouette\": [], \"cluster\": []}\n",
    "\n",
    "# выберем нужные данные \n",
    "X = df[['Attack', 'Defense']]\n",
    "\n",
    "for cluster_num in range(2, 10):\n",
    "    silhouette_res[\"silhouette\"].append(get_silhouette(cluster_num, X))\n",
    "    silhouette_res[\"cluster\"].append(cluster_num)\n",
    "    \n",
    "# сохраним в датафрейм значение силуэта и количество кластеров\n",
    "silhouette_df = pd.DataFrame(silhouette_res)\n",
    "\n",
    "# установим стиль для визуализиции\n",
    "sns.set_style(\"darkgrid\")\n",
    "# визуализируем зависимость значения инерции от количества кластеров\n",
    "sns.lineplot(data=silhouette_df, x=\"cluster\", y=\"silhouette\", marker= \"o\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM - АЛГОРИТМЫ КЛАСТЕРИЗАЦИИ\n",
    "* В основе данного подхода лежит предположение, что любой объект принадлежит ко всем кластерам, но с разной вероятностью.\n",
    "* Представителями являются:\n",
    "    * K-means кластеризация - например, для кластеризации документов\n",
    "    * GMM кластеризация - например, для сегментации изображения\n",
    "\n",
    "\n",
    "### КЛАСТЕРИЗАЦИЯ \"АЛГОРИТМ K-MEANS\"\n",
    "* Чувствительна к выбросам\n",
    "* K-MEANS - центроиды кластера это средние значения\n",
    "* K-MEANS++ - центроиды кластера это средние значения, но первые значения выбираются не случайно\n",
    "* K-MEDIANS - центроиды кластера это медианные значения\n",
    "* K-MEDOIDS - центроиды кластера это медианные значения, но не расчетное значение, а ближайшая к нему точка\n",
    "* FUZZY C-MEANS - каждый объект может принадлежать к разным кластерам с разной вероятностью\n",
    "``` python\n",
    "    from sklearn.cluster import KMeans\n",
    "    k_means_model = KMeans(n_clusters=2, init='k-means++', n_init=10, random_state=42)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем нужный модуль k-means-кластеризации\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# инициализируем алгоритм k-means с количеством кластеров 3\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, random_state=42)\n",
    "\n",
    "# запустим обучение модели\n",
    "kmeans.fit(X)\n",
    "\n",
    "# предскажем, к какому кластеру принадлежат покемоны \n",
    "predictions = kmeans.predict(X)\n",
    "# если мы хотим получить метки класса для тех же данных, на которых обучили модель, можно запросить labels\n",
    "predictions = kmeans.labels_\n",
    "\n",
    "# сохраним предсказания в датафрейм\n",
    "df['Clusters_k3'] = predictions\n",
    "\n",
    "#визуализируем результаты. Параметр c принимает вектор с номерами классов для группировки объектов по цветам \n",
    "sns.scatterplot(df.Attack, df.Defense, c=predictions)\n",
    "\n",
    "# KMeans(\n",
    "    # n_clusters — количество кластеров. По умолчанию — 8.\n",
    "    # init — способ инициализации центроидов. \n",
    "        # Есть две опции: random (выбирает центроиды случайным образом) \n",
    "        # и k-means++ (более «хитрый» алгоритм, который позволяет модели быстрее сходиться). По умолчанию используется k-means++.\n",
    "    # n_init — количество случайных инициализаций алгоритма k-means. \n",
    "        # В конце будут выбраны те результаты, которые имеют наилучшие значения критерия k-means. По умолчанию n_init = 10.\n",
    "    # max_iter — максимальное количество итераций алгоритма k-means при одном запуске. По умолчанию — 300.\n",
    "    # random_state=42 - гарантирует выдачу генератором одних и тех же случайных значений\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### КЛАСТЕРИЗАЦИЯ \"GMM\" - модель гауссовой смеси (Gaussian Mixture Model)\n",
    "* Чувствительна к выбросам\n",
    "``` python\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    gm_clst_model = GaussianMixture()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем библиотеки numpy и sklearn\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "gm_clustering = GaussianMixture(n_components=3, random_state=42)\n",
    "\n",
    "# обучаем модель \n",
    "gm_clustering.fit(X)\n",
    "\n",
    "# для матрицы X получаем предсказания к какому кластеру принадлежат объекты\n",
    "gm_prediction = gm_clustering.predict(X)\n",
    "\n",
    "# sns.set_style(\"white\")\n",
    "sns.scatterplot(df.Attack, df.Defense, c=gm_prediction)\n",
    "\n",
    "# GaussianMixture(\n",
    "    # n_components — количество кластеров.\n",
    "    # random_state=42 - гарантирует выдачу генератором одних и тех же случайных значений\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"ИЕРАРХИЧЕСКАЯ (АГЛОМЕРАТИВНАЯ)\" КЛАСТЕРИЗАЦИЯ\n",
    "* Принцип иерархической кластеризации основан на построении дерева (иерархии) вложенных кластеров.\n",
    "* Строится дендрограмма - это древовидная диаграмма, которая содержит  уровней. Каждый уровень — это шаг укрупнения кластеров.\n",
    "* Методы построения дендрограммы:\n",
    "    * Агломеративный метод (agglomerative) - объединение мелких кластеров\n",
    "    * Дивизионный (дивизивный) метод (divisive) - деление крупных кластеров\n",
    "* Методы расчета расстояния между кластерами:\n",
    "    * Метод одиночной связи_(min расстояние м/ж кластерами) `single`\n",
    "    * Метод полной связи____(max расстояние м/ж кластерами) `complete`\n",
    "    * Метод средней связи___(mean расстояние м/ж кластерами) `average`\n",
    "    * Центроидный метод_____(расстояние м/ж центрами кластеров) `ward`\n",
    "``` python\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    agg_clst_model = AgglomerativeClustering()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем алгомеративную кластеризацию из sklearn\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# запускаем агломеративную кластеризацию\n",
    "agglomerative_clustering = AgglomerativeClustering(n_clusters=2)\n",
    "\n",
    "# обучаем модель\n",
    "agglomerative_clustering.fit(X)\n",
    "\n",
    "# получаем метки c информацией, к какому кластеру относятся объекты\n",
    "agglomerative_clustering.labels_\n",
    "\n",
    "# AgglomerativeClustering(\n",
    "    # n_clusters — количество кластеров; по умолчанию — 2.\n",
    "    # linkage — метод определения расстояния между кластерами, которое мы рассматривали выше. \n",
    "        # Можно выбрать single, ward, average, complete; по умолчанию используется ward.    \n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### КЛАСТЕРИЗАЦИЯ НА ОСНОВЕПЛОТНОСТИ (DBSCAN)\n",
    "* DENSITY-BASED SPATIAL CLUSTERING OF APPLICATIONS WITH NOISE\n",
    "* В отличие от k-means, не нужно задавать количество кластеров — алгоритм сам определит оптимальное\n",
    "* Алгоритм хорошо работает с данными произвольной формы\n",
    "* DBSCAN отлично справляется с выбросами в датасетах\n",
    "``` python\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    clst_model = DBSCAN()    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем DBSCAN кластеризацию\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "#запускаем кластеризацию на наших данных\n",
    "clustering = DBSCAN(eps=3, min_samples=3).fit(df[['Attack', 'Defense']])\n",
    "\n",
    "# Далее можно визуализировать результаты, как мы делали с алгоритмом k-means\n",
    "sns.scatterplot(df.Attack, df.Defense, c=clustering.labels_)\n",
    "\n",
    "# DBSCAN(\n",
    "    # eps — это радиус, про который мы говорили выше, когда рассматривали алгоритм. \n",
    "        # Это один из важнейших параметров в алгоритме DBSCAN. \n",
    "        # Попробуйте изменять его и посмотрите, как будут меняться кластеры в зависимости от значения параметра. \n",
    "        # Если данные разрежённые, значение радиуса должно быть больше. \n",
    "        # Если же данные расположены довольно близко друг к другу, значение радиуса можно уменьшить.\n",
    "    # min_samples — задаёт необходимое количество точек, которое должно быть в заданном радиусе от точки, \n",
    "        # чтобы она считалась центральной.   \n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ПОНИЖЕНИЕ РАЗМЕРНОСТИ**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### СПЕКТРАЛЬНАЯ КЛАСТЕРИЗАЦИЯ\n",
    "* \n",
    "* Применяется для: \n",
    "* сегментации изображений\n",
    "``` python\n",
    "    from sklearn.cluster import SpectralClustering\n",
    "    spectral_clst_model = SpectralClustering()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вызываем из sklearn SpectralClustering \n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "\n",
    "# запускаем кластеризацию \n",
    "spectral_clustering = SpectralClustering(n_clusters=4, n_init=1500, random_state=42)\n",
    "\n",
    "spectral_clustering.fit(df[['Attack', 'Defense']])\n",
    "# получаем результаты кластеризации\n",
    "spectral_predictions = spectral_clustering.labels_\n",
    "\n",
    "sns.scatterplot( df.Attack, df.Defense, c=spectral_predictions)\n",
    "\n",
    "# SpectralClustering(\n",
    "    # n_clusters — количество кластеров; по умолчанию — 8.\n",
    "    # n_init — количество \n",
    "    # random_state=42 - гарантирует выдачу генератором одних и тех же случайных значений\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### PCA - линейное преобразование\n",
    "* Метод главных компонент, или PCA (Principal Components Analysis)\n",
    "* это один из базовых способов уменьшения размерности\n",
    "* Применяется для:  \n",
    "    * подавление шума\n",
    "    * индексация видео\n",
    "``` python\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# из модуля decomposition библиотеки sklearn импортируем класс PCA\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "# создадим объект класса PCA, уменьшим размерность данных до 2\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "\n",
    "# уменьшим размерность данных\n",
    "X_reduced = pca.fit_transform(train_x)\n",
    "# сохраним данные в датафрейм\n",
    "df_pca = pd.DataFrame(X_reduced)\n",
    "# сохраним разметки кластеров\n",
    "df_pca['c'] = pd.to_numeric(train_y).astype('Int64').to_list()\n",
    "# визуализируем\n",
    "sns.scatterplot(x=df_pca[0], y=df_pca[1], c=df_pca['c'])\n",
    "\n",
    "# PCA(\n",
    "    # n_components — размерность нового пространства.    \n",
    "    # random_state=42 - гарантирует выдачу генератором одних и тех же случайных значений\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE - нелинейное преобразование\n",
    "* t-SNE (t-distributed Stochastic Neighbor Embedding)\n",
    "* «стохастическое вложение соседей с t-распределением»\n",
    "* при преобразовании похожие объекты оказываются рядом, а непохожие — далеко друг от друга\n",
    "* Применяется для:  \n",
    "    * уменьшения размерность до 2х или 3х мерного\n",
    "``` python\n",
    "    from sklearn.manifold import TSNE\n",
    "    tsne = TSNE()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем класс TSNE из модуля manifold библиотеки sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "# создадим объект класса TSNE, уменьшим размерность данных до 2\n",
    "tsne = TSNE(n_components=2, perplexity=50, n_iter=500, random_state=42)\n",
    "# немного уменьшим количество объектов для уменьшения размерности, иначе алгоритм будет работать очень долго\n",
    "X_reduced = tsne.fit_transform(train_x)\n",
    "# сохраним данные в датафрейм\n",
    "df_tsne = pd.DataFrame(X_reduced)\n",
    "# сохраним разметки кластеров\n",
    "df_tsne['c'] = pd.to_numeric(train_y).astype('Int64').to_list()\n",
    "sns.scatterplot(x=df_tsne[0], y=df_tsne[1], c=df_tsne['c'])\n",
    "\n",
    "# TSNE(\n",
    "    # n_components — размерность нового пространства.\n",
    "    # perplexity — один из важнейших параметров для запуска. Этот параметр описывает ожидаемую плотность вокруг точки. \n",
    "        # Таким образом мы можем устанавливать соотношение ближайших соседей к точке. \n",
    "        # Если датасет большой, стоит установить большее значение perplexity. \n",
    "        # Обычно используют значения в диапазоне от 5 до 50.\n",
    "    # n_iter — количество итераций для оптимизации.\n",
    "    # random_state=42 - гарантирует выдачу генератором одних и тех же случайных значений\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ВИЗУАЛИЗАЦИЯ КЛАСТЕРИЗАЦИЙ\n",
    "* диаграмма рассеяния для двухмерного и трёхмерного случаев \n",
    "* Convex Hull, или выпуклая оболочка - провести гарницы\n",
    "* дендрограмма \n",
    "* Clustergram - \"полосы\" - только для иерархических кластеризаций"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотри: dst3-ml4-5_visualization.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### МЕТРИКИ КЛАСТЕРИЗАЦИИ \n",
    "``` python\n",
    "    from sklearn.metrics.cluster import homogeneity_score\n",
    "    from sklearn.metrics.cluster import completeness_score\n",
    "    from sklearn.metrics.cluster import v_measure_score\n",
    "    from sklearn.metrics.cluster import rand_score\n",
    "```\n",
    "* Однородность кластеров                - homogeneity_score(y, y_pred)\n",
    "* Полнота кластеров                     - completeness_score(y, y_pred)\n",
    "* V-мера (комбинация однор. и полн.)    - v_measure_score(y, y_pred)\n",
    "* Индекс Ренда                          - rand_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем подсчет метрики ОДНОРОДНОСИТ кластеров\n",
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "\n",
    "# теперь посчитаем насколько однородными получились кластеры с покемонами\n",
    "print(homogeneity_score(labels_true=df.RealClusters, labels_pred=df.Clusters_k3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем подсчет метрики ПОЛНОТЫ кластеров\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "\n",
    "# посчитаем насколько полными получились кластеры с покемонами\n",
    "print(completeness_score(labels_true=df.RealClusters, labels_pred=df.Clusters_k3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем из библиотеки sklearn подсчет V-меры\n",
    "from sklearn.metrics import v_measure_score\n",
    "# Эта метрика — комбинация метрик полноты и однородности.\n",
    "\n",
    "# теперь посчитаем v-меру для кластеров с покемонами\n",
    "print(v_measure_score(labels_true=df.RealClusters, labels_pred=df.Clusters_k3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем из библиотеки sklearn подсчет ИНДЕКСА РЭНДА\n",
    "from sklearn.metrics.cluster import rand_score\n",
    "\n",
    "# теперь посчитаем насколько полными получились кластеры с покемонами\n",
    "print(rand_score(labels_true=df.RealClusters, labels_pred=df.Clusters_k3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('sf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11 (default, Aug  6 2021, 09:57:55) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "563b3fbad9c1e703622b628cf34b11b58769da99b1ed9a9df4fdabd54b844cdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
