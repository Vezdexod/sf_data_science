{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# СОДЕРЖАНИЕ\n",
    "\n",
    "Образец запроса с описательной информацией\n",
    "``` python\n",
    "from sklearn.mixture import GaussianMixture\n",
    "help(GaussianMixture)\n",
    "```\n",
    "\n",
    "## ***РАЗДЕЛЕНИЕ ВЫБОРОК И ВАЛИДАЦИЯ***\n",
    "* Деление на 3 набора:\n",
    "    * Тренировочный X_train, y_train (70-80%)\n",
    "    * Валидационный X_valid, y_valid (10-15%)\n",
    "    * Тестовый      X_test,  y_test  (10-15%)\n",
    "\n",
    "* ### HOLD-OUT - \"ОТЛОЖЕННАЯ ВЫБОРКА\" \n",
    "    * Очень простой и понятный\n",
    "    * Чаще всего применяется на больших датасетов, так как менне ресурсный\n",
    "    * Можно улучшить стратификацией `stratify=y`\n",
    "    ``` python\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    #разбиваем исходную выборку на тренировочную и валидационную в соотношении 80/20\n",
    "    X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X, y, test_size=0.2)\n",
    "    #разбиваем валидационную выборку на валидационную и тестовую в соотношении 50/50\n",
    "    X_valid, X_test, y_valid, y_test = model_selection.train_test_split(X_valid, y_valid, test_size=0.5)\n",
    "    ```\n",
    "\n",
    "* ### K-FOLD - \"КРОСС-ВАЛИДАЦИЯ ИЛИ ПЕРЕКРЁСТНЫЙ КОНТРОЛЬ\"\n",
    "    * Исходые данные разбивают на k-фолдов (частей) с отделением тестовых данных\n",
    "    * Циклично итерируют фолды, при этом один из них по очереди является валидационным\n",
    "    * Получаем более точный модели, менее чувствительные к выбросам\n",
    "    * Более ресурсный и медленный, так как число итераций зависит от числа фолдов\n",
    "    * Если количество фолдов больше 30, можно построить доверительный интервал для среднего значения метрики\n",
    "    ``` python\n",
    "    from sklearn import model_selection\n",
    "    kf = model_selection.KFold() # Обычное разбиение\n",
    "    skf = model_selection.StratifiedKFold() # Стратифицированное разбиение\n",
    "    #Считаем метрики на кросс-валидации k-fold\n",
    "    cv_metrics = model_selection.cross_validate()\n",
    "    print('Train k-fold mean accuracy: {:.2f}'.format(np.mean(cv_metrics['train_score'])))\n",
    "    ```\n",
    "\n",
    "* ### LIVE-ONE-OUT - \"ОТЛОЖЕННЫЙ ПРИМЕР ИЛИ ПОЭЛЕМЕНТНАЯ КРОСС-ВАЛИДАЦИЯ\"\n",
    "    * Алгоритм:\n",
    "        * Повторять n раз:\n",
    "            * Выбрать один случайный пример для валидации\n",
    "            * Обучить модель на всех оставшихся  примерах\n",
    "            * Произвести оценку качества (вычислить метрику) на отложенном примере\n",
    "        * Усреднить значение метрик на всех примерах\n",
    "    * Имеет наиболее объективные и надёжные метрики\n",
    "    * Идеален для небольших данных (порядка 100)\n",
    "    * Очень ресурсозатратный\n",
    "    ``` python\n",
    "    from sklearn import model_selection\n",
    "    loo = model_selection.LeaveOneOut()\n",
    "    cv_metrics = model_selection.cross_validate()\n",
    "    print('Train k-fold mean accuracy: {:.2f}'.format(np.mean(cv_metrics['train_score'])))\n",
    "    ```\n",
    "    * Примечание. Метод leave-one-out можно реализовать и без использования специального класса — достаточно просто указать параметр n_split=n в инициализаторе KFold, где n — количество строк в таблице.\n",
    "\n",
    "* ### БОРЬБА С ДИСБАЛАНСОМ\n",
    "    * Взвешивание объектов. В функцию ошибки добавляется штраф, прямо пропорциональный количеству объектов каждого класса. \n",
    "    * Выбор порога вероятности. Заключается в том, что мы подбираем такой порог вероятности (по умолчанию он равен 0.5 во всех моделях), при котором на валидационной выборке максимизируется целевая метрика.\n",
    "    * Сэмплирование (sampling) — перебалансировка выборки искусственным путём:  \n",
    "        * oversampling — искусственное увеличение количества объектов миноритарного класса;\n",
    "        * undersampling — сокращение количества объектов мажоритарного класса:\n",
    "            Здесь могут использоваться алгоритмы генерации искусственных данных, такие как NearMiss, SMOTE (Synthetic Minority Oversampling Techniques) и ADASYN (Adaptive Synthetic).\n",
    "            Мы рассмотрим наиболее популярный алгоритм — SMOTE\n",
    "        ``` python\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        sm = SMOTE(random_state=42)\n",
    "        X_train_s, y_train_s = sm.fit_resample(X_train, y_train)\n",
    "        ```\n",
    "---\n",
    "\n",
    "## ***РЕГРЕССИЯ***\n",
    "\n",
    "* ### АНАЛИТИЧЕСКОЕ РЕШЕНИЕ (NumPy)\n",
    "\n",
    "* ### АНАЛИТИЧЕСКОЕ РЕШЕНИЕ \"Линейная регрессия\" (sklearn)\n",
    "    ``` python\n",
    "    from sklearn import linear_model    \n",
    "    lr_model = linear_model.LinearRegression()\n",
    "    ```\n",
    "\n",
    "* ### ЧИСЛОВОЕ РЕШЕНИЕ \"Градиентный спуск (Gradient descent)\" (sklearn)\n",
    "    ``` python\n",
    "    from sklearn import linear_model\n",
    "    sgd_lr_model = linear_model.SGDRegressor()\n",
    "    ```\n",
    "\n",
    "* ### ПОЛИНОМИАЛЬНЫЕ ПРИЗНАКИ (Полиномиальная регрессия (Polynomial Regression))\n",
    "    * Нужно проводить стандартизацию и нормальзацию до этого этапа\n",
    "    ``` python\n",
    "    from sklearn import preprocessing\n",
    "    poly = preprocessing.PolynomialFeatures(degree=2, include_bias=False)\n",
    "    poly.fit(X_train)\n",
    "    X_train_poly = poly.transform(X_train)\n",
    "    ```\n",
    "    \n",
    "* ### РЕГУЛЯРИЗАЦИЯ (борьба с разбросом (переобучением)) \n",
    "    * L1-регуляризация (Lasso)\n",
    "        ``` python\n",
    "        lasso_lr_poly = linear_model.Lasso(alpha=0.1)\n",
    "        lasso_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "        ```\n",
    "    * L2-регуляризация (Ridge), или регуляризация Тихонова\n",
    "        ``` python\n",
    "        ridge_lr_poly = linear_model.Ridge(alpha=10)\n",
    "        ridge_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "        ```\n",
    "\n",
    "* ### МЕТРИКИ РЕГРЕССИИ \n",
    "    ``` python\n",
    "    from sklearn import metrics\n",
    "    ```\n",
    "    * MAE (Mean Absolute Error)             - mean_absolute_error(y, y_pred)\n",
    "    * MAPE (Mean Absolute Percent Error)    - mean_absolute_percentage_error(y, y_pred)*100\n",
    "    * MSE (Mean Square Error)               - mean_square_error(y, y_pred)\n",
    "    * RMSE (Root Mean Squared Error)        - np.sqrt(mean_square_error(y, y_pred))\n",
    "    * R2 (Коэффициент детерминации)         - r2_score(y, y_pred)\n",
    "\n",
    "---\n",
    "\n",
    "## ***КЛАССИФИКАЦИЯ***\n",
    "\n",
    "* ### ЧИСЛОВОЕ РЕШЕНИЕ \"Логистическая регрессия\"\n",
    "    ``` python\n",
    "    from sklearn import linear_model\n",
    "    log_reg_model = linear_model.LogisticRegression()\n",
    "    ```\n",
    "\n",
    "* ### МУЛЬТИКЛАССОВАЯ КЛАССИФИКАЦИЯ (когда не 2 класса, а больше)\n",
    "    * Сравнивает класс 0 с классом 1 и 2, потом класс 1 с классами 0 и 2 и т. д.\n",
    "    ``` python\n",
    "    from sklearn import linear_model\n",
    "    log_reg_model = linear_model.LogisticRegression(\n",
    "        multi_class='multinomial', #мультиклассовая классификация\n",
    "    )\n",
    "    ```\n",
    "\n",
    "* ### ДЕРЕВЬЯ РЕШЕНИЙ (связанный ациклический граф)\n",
    "    * состоит из корневой вершины, внутренних вершин и листьев\n",
    "    * Предикаты - критерии находящиеся на вершинах\n",
    "    ``` python\n",
    "    from sklearn import tree # Модели деревьев решения\n",
    "    # Создаём объект класса DecisionTreeClassifier\n",
    "    dt_clf_model = tree.DecisionTreeClassifier()\n",
    "    dt_clf_model.get_depth() # Показывает дерево\n",
    "    ```\n",
    "\n",
    "* ### АНСАМБЛИ (БЕГГИНГ) \"Случайный лес\"\n",
    "    * Много слабых моделей объединяются в одну\n",
    "    * Виды ансамблей \n",
    "        * Бэггинг — параллельно обучаем множество одинаковых моделей, а для предсказания берём среднее по предсказаниям каждой из моделей.\n",
    "        * Бустинг — последовательно обучаем множество одинаковых моделей, где каждая новая модель концентрируется на тех примерах, где предыдущая допустила ошибку.\n",
    "        * Стекинг — параллельно обучаем множество разных моделей, отправляем их результаты в финальную модель, и уже она принимает решение.\n",
    "    ``` python \n",
    "    from sklearn import ensemble\n",
    "    rf_clf_model = ensemble.RandomForestClassifier(\n",
    "        n_estimators=500, #число деревьев\n",
    "    )\n",
    "    ```\n",
    "\n",
    "* ### МЕТРИКИ КЛАССИФИКАЦИИ \n",
    "    ```python\n",
    "    from sklearn import metrics\n",
    "    ```\n",
    "    * Матрица ошибок                        - confusion_matrix(y, y_pred)\n",
    "    * Accuracy (достоверность)              - accuracy_score(y, y_pred)\n",
    "        * чем ближе к 1, тем больше угадала\n",
    "    * Precision (точность) или              - precision_score(y, y_pred)\n",
    "        * PPV (Positive Predictive Value)\n",
    "        * чем ближе к 1, тем меньше ошибок\n",
    "    * Recall (полнота) или                  - recall_score(y, y_pred)\n",
    "        * TPR (True Positive Rate)\n",
    "        * чем ближе к 1, тем больше значений 1 класса названо правильно\n",
    "    * F-мера                                - f1_score(y, y_pred)\n",
    "        * (это взвешенное среднее гармоническое между precision и recall)\n",
    "        * чем ближе к 1, тем точнее модель\n",
    "    * Все ошибки в одном отчете             - classification_report(y, y_pred)\n",
    "\n",
    "---\n",
    "\n",
    "## ***КЛАСТЕРИЗАЦИЯ***\n",
    "\n",
    "* ### ОПРЕДЕЛЕНИЕ ОПТИМАЛЬНОГО КОЛИЧЕСТВА КЛАСТЕРОВ\n",
    "    * #### МЕТОД ЛОКТЯ - построение графика зависимости инерции от количества кластеров\n",
    "        ``` python\n",
    "        model.inertia_ # получение инерции для создания графика\n",
    "        ```\n",
    "\n",
    "    * #### КОЭФФИЦИЕНТ СИЛУЭТА - построение графика зависимости коэффициента силуэта от количества кластеров\n",
    "        * Коэффициент силуэта показывает, насколько объект похож на объекты кластера, в котором он находится, по сравнению с объектами из других кластеров.\n",
    "        ``` python\n",
    "        silhouette_score(X, model.labels_)\n",
    "        ```\n",
    "* ### EM - АЛГОРИТМЫ КЛАСТЕРИЗАЦИИ\n",
    "    * В основе данного подхода лежит предположение, что любой объект принадлежит ко всем кластерам, но с разной вероятностью.\n",
    "    * Представителями являются:\n",
    "        * K-means кластеризация - например, для кластеризации документов\n",
    "        * GMM кластеризация - например, для сегментации изображения\n",
    "\n",
    "\n",
    "* ### КЛАСТЕРИЗАЦИЯ \"АЛГОРИТМ K-MEANS\"\n",
    "    * Чувствительна к выбросам\n",
    "    * K-MEANS - центроиды кластера это средние значения\n",
    "    * K-MEANS++ - центроиды кластера это средние значения, но первые значения выбираются не случайно\n",
    "    * K-MEDIANS - центроиды кластера это медианные значения\n",
    "    * K-MEDOIDS - центроиды кластера это медианные значения, но не расчетное значение, а ближайшая к нему точка\n",
    "    * FUZZY C-MEANS - каждый объект может принадлежать к разным кластерам с разной вероятностью\n",
    "    ``` python\n",
    "    from sklearn.cluster import KMeans\n",
    "    k_means_model = KMeans(n_clusters=2, init='k-means++', n_init=10, random_state=42)\n",
    "    ```\n",
    "* ### КЛАСТЕРИЗАЦИЯ \"GMM\" - модель гауссовой смеси (Gaussian Mixture Model)\n",
    "    * Чувствительна к выбросам\n",
    "    ``` python\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    gm_clst_model = GaussianMixture()\n",
    "    ```\n",
    "\n",
    "* ### \"ИЕРАРХИЧЕСКАЯ\" КЛАСТЕРИЗАЦИЯ\n",
    "    * Принцип иерархической кластеризации основан на построении дерева (иерархии) вложенных кластеров.\n",
    "    * Строится дендрограмма - это древовидная диаграмма, которая содержит  уровней. Каждый уровень — это шаг укрупнения кластеров.\n",
    "    * Методы построения дендрограммы:\n",
    "        * Агломеративный метод (agglomerative) - объединение мелких кластеров\n",
    "        * Дивизионный (дивизивный) метод (divisive) - деление крупных кластеров\n",
    "    * Методы расчета расстояния между кластерами\n",
    "        * Метод одиночной связи (min расстояние)\n",
    "        * Метод полной связи    (max расстояние)\n",
    "        * Метод средней связи   (mean расстояние)\n",
    "        * Центроидный метод     (расстояние м/ж центрами)\n",
    "    ``` python\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    agg_clst_model = AgglomerativeClustering()\n",
    "    ```\n",
    "\n",
    "* ### **ПОНИЖЕНИЕ РАЗМЕРНОСТИ**\n",
    "    * #### СПЕКТРАЛЬНАЯ КЛАСТЕРИЗАЦИЯ\n",
    "        * \n",
    "        * Применяется для: \n",
    "            * сегментации изображений\n",
    "        ``` python\n",
    "        from sklearn.cluster import SpectralClustering\n",
    "        spectral_clst_model = SpectralClustering()\n",
    "        ```\n",
    "\n",
    "    * #### PCA - линейное преобразование\n",
    "        * Метод главных компонент, или PCA (Principal Components Analysis)\n",
    "        * это один из базовых способов уменьшения размерности\n",
    "        * Применяется для:  \n",
    "            * подавление шума\n",
    "            * индексация видео\n",
    "        ``` python\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA()\n",
    "        ```\n",
    "\n",
    "    * #### t-SNE - нелинейное преобразование\n",
    "        * t-SNE (t-distributed Stochastic Neighbor Embedding)\n",
    "        * «стохастическое вложение соседей с t-распределением»\n",
    "        * при преобразовании похожие объекты оказываются рядом, а непохожие — далеко друг от друга\n",
    "        * Применяется для:  \n",
    "            * уменьшения размерность до 2х или 3х мерного\n",
    "        ``` python\n",
    "        from sklearn.manifold import TSNE\n",
    "        tsne = TSNE()\n",
    "        ```\n",
    "\n",
    "* ### КЛАСТЕРИЗАЦИЯ НА ОСНОВЕПЛОТНОСТИ (DBSCAN)\n",
    "    * DENSITY-BASED SPATIAL CLUSTERING OF APPLICATIONS WITH NOISE\n",
    "    * В отличие от k-means, не нужно задавать количество кластеров — алгоритм сам определит оптимальное\n",
    "    * Алгоритм хорошо работает с данными произвольной формы\n",
    "    * DBSCAN отлично справляется с выбросами в датасетах\n",
    "    ``` python\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    clst_model = DBSCAN()    \n",
    "    ```\n",
    "* ### ВИЗУАЛИЗАЦИЯ КЛАСТЕРИЗАЦИЙ\n",
    "    * диаграмма рассеяния для двухмерного и трёхмерного случаев \n",
    "    * Convex Hull, или выпуклая оболочка - провести гарницы\n",
    "    * дендрограмма \n",
    "    * Clustergram - \"полосы\" - только для иерархических кластеризаций\n",
    "\n",
    "* ### МЕТРИКИ КЛАСТЕРИЗАЦИИ \n",
    "    ``` python\n",
    "    from sklearn.metrics.cluster import homogeneity_score\n",
    "    from sklearn.metrics.cluster import completeness_score\n",
    "    from sklearn.metrics.cluster import v_measure_score\n",
    "    from sklearn.metrics.cluster import rand_score\n",
    "    ```\n",
    "    * Однородность кластеров                - homogeneity_score(y, y_pred)\n",
    "    * Полнота кластеров                     - completeness_score(y, y_pred)\n",
    "    * V-мера (комбинация однор. и полн.)    - v_measure_score(y, y_pred)\n",
    "    * Индекс Ренда                          - rand_score(y, y_pred)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Базовые библиотеки\n",
    "import numpy as np # для матричных вычислений\n",
    "import pandas as pd # для анализа и предобработки данных\n",
    "import matplotlib.pyplot as plt # для визуализации\n",
    "import seaborn as sns # для визуализации\n",
    "\n",
    "# Модели\n",
    "from sklearn import linear_model # линейные модели\n",
    "from sklearn import tree # деревья\n",
    "from sklearn.cluster import KMeans # KMeans кластеризация\n",
    "from sklearn.mixture import GaussianMixture # GMM кластеризация\n",
    "from sklearn.cluster import SpectralClustering # Спектральная кластеризация\n",
    "from sklearn.decomposition import PCA # PCA снижение размерности\n",
    "from sklearn.manifold import TSNE # T-SNE снижение размерности\n",
    "from sklearn.cluster import DBSCAN # DBSCAN кластеризация\n",
    "\n",
    "# Метрики регрессии и классификации\n",
    "from sklearn import metrics # метрики \n",
    "\n",
    "# Метрики кластеризации\n",
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "from sklearn.metrics.cluster import rand_score\n",
    "\n",
    "\n",
    "# Работа с данными для моделей\n",
    "from sklearn.model_selection import train_test_split # сплитование выборки\n",
    "from sklearn import model_selection # для K-Fold и LIVE-ONE-OUT\n",
    "from sklearn import ensemble # ансамбли\n",
    "\n",
    "#from imblearn.over_sampling import SMOTE # Для сэмплирования (придумывания) данных\n",
    "from sklearn import preprocessing # предобработка\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('pokemon.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# матрица наблюдений и вектор правильных ответов\n",
    "X, y = data.drop('RealClusters', axis=1), data['RealClusters'] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***РАЗДЕЛЕНИЕ ВЫБОРОК И ВАЛИДАЦИЯ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOLD-OUT - \"ОТЛОЖЕННАЯ ВЫБОРКА\"\n",
    "* Деление на 3 набора:\n",
    "    * Тренировочный X_train, y_train (70-80%)\n",
    "    * Валидационный X_valid, y_valid (10-15%)\n",
    "    * Тестовый      X_test,  y_test  (10-15%)\n",
    "* Очень простой и понятный\n",
    "* Чаще всего применяется на больших датасетов, так как менне ресурсный\n",
    "* Можно улучшить стратификацией `stratify=y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Разделяем выборку на тренировочную и валидационную в соотношении 70/30 со стратификацией\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Разделяем выборку на валидационную и тестовую в соотношении 50/50 со стратификацией\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=0.5, stratify=y_valid, random_state=42)\n",
    "\n",
    "# Выводим результирующие размеры таблиц\n",
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Valid:', X_valid.shape, y_valid.shape)\n",
    "print('Test:', X_test.shape, y_test.shape)\n",
    "\n",
    "# train_test_split(\n",
    "    # X - матрица признаков (наблюдений)\n",
    "    # y - вектор правильных ответов\n",
    "    # test_size=0.3 - 30% данных идут в тестовую выборку\n",
    "    # train_size - размер тренировочной выборки. Может быть указан в долях. \n",
    "        # Определяется автоматически, если параметр test_size передан как 1-test_size.    \n",
    "    # shuffle=False - По умолчанию True. Параметр перемешивания данных в выборке.\n",
    "    # stratify=y - Стратифицированное разбиение - Одинаковое соотношение целевого признака \n",
    "        # в тренировочной и тестовой выборке, не допускающее перекоса в обучении модели.\n",
    "    # random_state=42 - гарантирует выдачу генератором одних и тех же случайных значений\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-FOLD - \"КРОСС-ВАЛИДАЦИЯ ИЛИ ПЕРЕКРЁСТНЫЙ КОНТРОЛЬ\"\n",
    "* Исходые данные разбивают на k-фолдов (частей) с отделением тестовых данных\n",
    "* Циклично итерируют фолды, при этом один из них по очереди является валидационным\n",
    "* Получаем более точный модели, менее чувствительные к выбросам\n",
    "* Более ресурсный и медленный, так как число итераций зависит от числа фолдов\n",
    "* Если количество фолдов больше 30, можно построить доверительный интервал для среднего значения метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "# Создаём кросс-валидатор KFold\n",
    "kf = model_selection.KFold(n_splits=5) # Обычное разбиение\n",
    "skf = model_selection.StratifiedKFold(n_splits=5) # Стратифицированное разбиение\n",
    "\n",
    "# Создаём кросс-валидатор LeaveOneOut\n",
    "loo = model_selection.LeaveOneOut()\n",
    " \n",
    "#Считаем метрики на кросс-валидации k-fold\n",
    "cv_metrics = model_selection.cross_validate(\n",
    "    estimator=model, #модель\n",
    "    X=X, #матрица наблюдений X\n",
    "    y=y, #вектор ответов y\n",
    "    cv=kf, #кросс-валидатор KFold\n",
    "    #cv=skf, #кросс-валидатор SKFold\n",
    "    #cv=loo, #кросс-валидатор LeaveOneOut\n",
    "    scoring='accuracy', #метрика\n",
    "    return_train_score=True #подсчёт метрики на тренировочных фолдах\n",
    ")\n",
    "\n",
    "print('Train k-fold mean f1: {:.2f}'.format(np.mean(cv_metrics['train_score'])))\n",
    "print('Valid k-fold mean f1: {:.2f}'.format(np.mean(cv_metrics['test_score'])))\n",
    "\n",
    "#Делаем предсказание вероятностей на кросс-валидации\n",
    "y_cv_proba_pred = model_selection.cross_val_predict(model, X_train, y_train, cv=skf, method='predict_proba')\n",
    "\n",
    "#Выделяем столбец с вероятностями для класса 0\n",
    "y_cv_proba_pred_0 = y_cv_proba_pred[:, 0]\n",
    "\n",
    "#Выделяем столбец с вероятностями для класса 1 \n",
    "y_cv_proba_pred_1 = y_cv_proba_pred[:, 1]\n",
    "\n",
    "# KFold(\n",
    "    # n_splits=5 - число фолдов (частей)\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ВЫБОР ПОРОГА ВЕРОЯТНОСТИ. PR-КРИВАЯ.\n",
    "PR-кривая (precision-recall curve) — это график зависимости precision от recall при различных значениях порога вероятности.\n",
    "\n",
    "Мы можем построить PR-кривую. Для этого воспользуемся функций precision_recall_curve() из модуля metrics библиотеки sklearn. В данную функцию нужно передать истинные метки классов и предсказанные вероятности. Взамен она вернёт три массива: значения метрик precision и recall, вычисленных на различных порогах вероятности, и сами пороги вероятности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Вычисляем координаты PR-кривой\n",
    "precision, recall, thresholds = metrics.precision_recall_curve(y_train, y_cv_proba_pred)\n",
    "\n",
    "#Вычисляем F1-score при различных threshold\n",
    "f1_scores = (2 * precision * recall) / (precision + recall)\n",
    "#Определяем индекс максимума\n",
    "idx = np.argmax(f1_scores)\n",
    "print('Best threshold = {:.2f}, F1-Score = {:.2f}'.format(thresholds[idx], f1_scores[idx]))\n",
    " \n",
    "#Строим PR-кривую\n",
    "fig, ax = plt.subplots(figsize=(10, 5)) #фигура + координатная плоскость\n",
    "#Строим линейный график зависимости precision от recall\n",
    "ax.plot(precision, recall, label='Decision Tree PR')\n",
    "#Отмечаем точку максимума F1\n",
    "ax.scatter(precision[idx], recall[idx], marker='o', color='black', label='Best F1 score')\n",
    "#Даём графику название и подписываем оси\n",
    "ax.set_title('Precision-recall curve')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "#Отображаем легенду\n",
    "ax.legend();    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем предсказание классов с таким порогом для всех объектов из отложенной валидационной выборки и выведем отчёт о метриках:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Образцы воды, для которых вероятность быть пригодными для питья > threshold_opt, относим к классу 1\n",
    "#В противном случае — к классу 0\n",
    "y_valid_pred_proba = model.predict_proba(X_valid)[:, 1]\n",
    "y_valid_pred = (y_valid_pred_proba > threshold_opt).astype('int')\n",
    "#Считаем метрики\n",
    "print(metrics.classification_report(y_valid, y_valid_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## СЭМПЛИРОВАНИЕ\n",
    "\n",
    "Следующий подход работы в условиях дисбаланса классов, который мы рассмотрим, — сэмплирование, а точнее — пересэмплирование (oversampling).\n",
    "\n",
    "Идея очень проста: если у нас мало наблюдений миноритарного класса, следует искусственно увеличить их количество."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для пользователей pip:\n",
    "\n",
    "`!pip install imbalanced-learn`\n",
    "Для пользователей anaconda:\n",
    "\n",
    "`!conda install -c conda-forge imbalanced-learn`\n",
    "Все алгоритмы пересэмплирования находятся в модуле over_sampling библиотеки imblearn. Импортируем оттуда алгоритм SMOTE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим объект класса SMOTE и вызовем у него метод fit_sample(), передав в него обучающую выборку (X_train, y_train). Затем выведем количество наблюдений каждого из классов до и после сэмплирования:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ТОЛЬКО ДЛЯ ТРЕНИРОВОЧНОЙ ВЫБОРКИ!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_s, y_train_s = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "print('Train shape before oversampling:', X_train.shape) \n",
    "print('Class balance before oversampling: \\n', y_train.value_counts(), sep='')\n",
    "print('-'*40)\n",
    "print('Train shape after oversampling:', X_train_s.shape)\n",
    "print('Class balance after oversampling: \\n', y_train_s.value_counts(), sep='')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## КРИВАЯ ОБУЧЕНИЯ (learning curve) \n",
    "* это график зависимости некоторой метрики на обучающем (валидационном) наборе данных от количества объектов, которые участвуют в обучении модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаём объект кросс-валидатора k-fold со стратификацией\n",
    "skf = model_selection.StratifiedKFold(n_splits=5)\n",
    " \n",
    "#Вычисляем координаты для построения кривой обучения\n",
    "train_sizes, train_scores, valid_scores = model_selection.learning_curve(\n",
    "    estimator = model, #модель\n",
    "    X = X, #матрица наблюдений X\n",
    "    y = y, #вектор ответов y\n",
    "    cv = skf, #кросс-валидатор\n",
    "    scoring = 'f1' #метрика\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(model, X, y, cv, scoring=\"f1\", ax=None, title=\"\"):\n",
    "    # Вычисляем координаты для построения кривой обучения\n",
    "    train_sizes, train_scores, valid_scores = model_selection.learning_curve(\n",
    "        estimator=model,  # модель\n",
    "        X=X,  # матрица наблюдений X\n",
    "        y=y,  # вектор ответов y\n",
    "        cv=cv,  # кросс-валидатор\n",
    "        scoring=scoring,  # метрика\n",
    "    )\n",
    "    # Вычисляем среднее значение по фолдам для каждого набора данных\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    valid_scores_mean = np.mean(valid_scores, axis=1)\n",
    "    # Если координатной плоскости не было передано, создаём новую\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))  # фигура + координатная плоскость\n",
    "    # Строим кривую обучения по метрикам на тренировочных фолдах\n",
    "    ax.plot(train_sizes, train_scores_mean, label=\"Train\")\n",
    "    # Строим кривую обучения по метрикам на валидационных фолдах\n",
    "    ax.plot(train_sizes, valid_scores_mean, label=\"Valid\")\n",
    "    # Даём название графику и подписи осям\n",
    "    ax.set_title(\"Learning curve: {}\".format(title))\n",
    "    ax.set_xlabel(\"Train data size\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    # Устанавливаем отметки по оси абсцисс\n",
    "    ax.xaxis.set_ticks(train_sizes)\n",
    "    # Устанавливаем диапазон оси ординат\n",
    "    ax.set_ylim(0, 1)\n",
    "    # Отображаем легенду\n",
    "    ax.legend()\n",
    "    \n",
    "# plot_learning_curve(\n",
    "    # estimator — модель, качество которой будет проверяться на кросс-валидации.\n",
    "    # X — матрица наблюдений.\n",
    "    # y — вектор-столбец правильных ответов.\n",
    "    # train_sizes — относительное (долевое) или абсолютное количество обучающих примеров, которые будут \n",
    "        # использоваться для создания кривой обучения. Если dtype имеет значение float, он \n",
    "        # рассматривается как часть максимального размера обучающего набора (который определяется выбранным методом проверки), \n",
    "        # т. е. он должен быть в пределах (0, 1].\n",
    "        # По умолчанию используется список [0.1, 0.325, 0.55, 0.775, 1.0], то есть для построения кривой обучения \n",
    "        # используется пять точек. Первая точка кривой обучения строится по 10 % наблюдений из обучающего набора, \n",
    "        # вторая точка — по 32.5 % и так далее до тех пор, пока в построении модели не будет участвовать весь обучающий набор данных.\n",
    "    # cv — кросс-валидатор из библиотеки sklearn (например, KFold) или количество фолдов, на которые необходимо разбить выборку. \n",
    "        # По умолчанию используется кросс-валидация k-fold на пяти фолдах.\n",
    "    # scoring — название метрики в виде строки либо функция для её вычисления.\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно сравниавть метрики разных моделей, загрузив их в список `models`.\n",
    "``` python\n",
    "    models = [\n",
    "        model_1,\n",
    "        ...,\n",
    "        model_n\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаём объект кросс-валидатора k-fold со стратификацией\n",
    "skf = model_selection.StratifiedKFold(n_splits=5)\n",
    "#Визуализируем кривые обучения\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4)) #фигура + три координатных плоскости\n",
    "#Создаём цикл по списку моделей и индексам этого списка\n",
    "for i, model in enumerate(models): #i — текущий индекс, model — текущая модель\n",
    "    plot_learning_curve(model, X, y, skf, ax=axes[i], title=f'model {i+1}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***РЕГРЕССИЯ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### АНАЛИТИЧЕСКОЕ РЕШЕНИЕ (NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y):\n",
    "    # Создаём вектор из единиц\n",
    "    ones = np.ones(X.shape[0])\n",
    "    # Добавляем вектор к таблице первым столбцом\n",
    "    X = np.column_stack([ones, X])\n",
    "    # Вычисляем обратную матрицу Q\n",
    "    Q = np.linalg.inv(X.T @ X)\n",
    "    # Вычисляем вектор коэффициентов\n",
    "    w = Q @ X.T @ y\n",
    "    return w\n",
    "# Вычисляем параметры линейной регрессии\n",
    "w = linear_regression(X, y)\n",
    "# Выводим вычисленные значения параметров в виде вектора\n",
    "print('Vector w: {}'.format(w))\n",
    "# Выводим параметры с точностью до двух знаков после запятой\n",
    "print('w0 = {:.2f}'.format(w[0]))\n",
    "print('w1 = {:.2f}'.format(w[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### АНАЛИТИЧЕСКОЕ РЕШЕНИЕ \"Линейная регрессия\" (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём объект класса LinearRegression\n",
    "lr_model = linear_model.LinearRegression()\n",
    "\n",
    "# Обучаем модель — ищем параметры по МНК\n",
    "lr_model.fit(X, y) \n",
    "\n",
    "# Выводим полученные параметры\n",
    "print('w0 = {}'.format(lr_model.intercept_)) #свободный член w0\n",
    "print('w1 = {}'.format(lr_model.coef_)) #остальные параметры модели w1, w2, ..., wm\n",
    "\n",
    "# Предсказываем медианную цену для всех участков из набора данных\n",
    "y_predict = lr_model.predict(X)\n",
    "\n",
    "# Выводим предсказание\n",
    "# Составляем таблицу из признаков и их коэффициентов\n",
    "w_df = pd.DataFrame({'Features': X.columns, 'Coefficients': lr_model.coef_})\n",
    "# Составляем строку таблицы со свободным членом\n",
    "intercept_df =pd.DataFrame({'Features': ['INTERCEPT'], 'Coefficients': lr_model.intercept_})\n",
    "coef_df = pd.concat([w_df, intercept_df], ignore_index=True)\n",
    "display(coef_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ЧИСЛОВОЕ РЕШЕНИЕ \"Градиентный спуск (Gradient descent)\" (sklearn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто требуется стандартизация данных, для коррекции масштаба. Предсказания на валидационных и тестовых выборках нужно делать так же после стандартизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    " \n",
    "# Инициализируем стандартизатор StandardScaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Подгоняем параметры стандартизатора (вычисляем среднее и СКО)\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Производим стандартизацию тренировочной выборки\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Производим стандартизацию тестовой выборки\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём объект класса линейной регрессии с SGD\n",
    "sgd_lr_model = linear_model.SGDRegressor(random_state=42)\n",
    "\n",
    "# Обучаем модель — ищем параметры по методу SGD\n",
    "sgd_lr_model.fit(X, y)\n",
    "\n",
    "# Выводим полученные параметры\n",
    "print('w0: {}'.format(sgd_lr_model.intercept_)) #свободный член w0\n",
    "print('w1: {}'.format(sgd_lr_model.coef_)) #остальные параметры модели w1, w2, ..., wm\n",
    "\n",
    "#Предсказываем медианную цену для всех участков из набора данных\n",
    "y_predict = sgd_lr_model.predict(X)\n",
    "\n",
    "# Выводим предсказание\n",
    "# Составляем таблицу из признаков и их коэффициентов\n",
    "w_df = pd.DataFrame({'Features': X.columns, 'Coefficients': sgd_lr_model.coef_})\n",
    "# Составляем строку таблицы со свободным членом\n",
    "intercept_df =pd.DataFrame({'Features': ['INTERCEPT'], 'Coefficients': sgd_lr_model.intercept_})\n",
    "coef_df = pd.concat([w_df, intercept_df], ignore_index=True)\n",
    "display(coef_df)\n",
    "\n",
    "# SGDRegressor(\n",
    "    # loss — функция потерь. По умолчанию используется squared_loss — уже привычная нам MSE. \n",
    "        # Но могут использоваться и несколько других. Например, значение \"huber\" определяет функцию потерь Хьюбера. \n",
    "        # Эта функция менее чувствительна к наличию выбросов, чем MSE.\n",
    "    # max_iter — максимальное количество итераций, выделенное на сходимость. Значение по умолчанию — 1000.\n",
    "    # learning_rate — режим управления темпом обучения. Значение по умолчанию — 'invscaling'. \n",
    "        # Этот режим уменьшает темп обучения по формуле, которую мы рассматривали ранее: etat=eta0/t^p.\n",
    "        # Есть ещё несколько режимов управления, о которых вы можете прочитать в документации.\n",
    "        # Если вы не хотите, чтобы темп обучения менялся на протяжении всего обучения, \n",
    "        # то можете выставить значение параметра на \"constant\".\n",
    "    # eta0 — начальное значение темпа обучения . Значение по умолчанию — 0.01.\n",
    "        # Если параметр learning_rate=\"constant\", то значение этого параметра \n",
    "        # будет темпом обучения на протяжении всех итераций.\n",
    "    # power_t — значение мощности уменьшения  в формуле etat=eta0/t^p . Значение по умолчанию — 0.25.\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ПОЛИНОМИАЛЬНЫЕ ПРИЗНАКИ (Полиномиальная регрессия (Polynomial Regression))\n",
    "* Нужно проводить стандартизацию и нормальзацию до этого этапа\n",
    "``` python\n",
    "    from sklearn import preprocessing\n",
    "    poly = preprocessing.PolynomialFeatures(degree=2, include_bias=False)\n",
    "    poly.fit(X_train)\n",
    "    X_train_poly = poly.transform(X_train)\n",
    "```\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стандартизацию (нормализацию) полезнее проводить перед \n",
    "генерацией полиномиальных признаков, иначе можно потерять масштаб полиномов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    " \n",
    "# Инициализируем стандартизатор StandardScaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Подгоняем параметры стандартизатора (вычисляем среднее и СКО)\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Производим стандартизацию тренировочной выборки\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Производим стандартизацию тестовой выборки\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаём генератор полиномиальных признаков\n",
    "poly = preprocessing.PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly.fit(X_train_scaled)\n",
    "#Генерируем полиномиальные признаки для тренировочной выборки\n",
    "X_train_scaled_poly = poly.transform(X_train_scaled)\n",
    "#Генерируем полиномиальные признаки для тестовой выборки\n",
    "X_test_scaled_poly = poly.transform(X_test_scaled)\n",
    "#Выводим результирующие размерности таблиц\n",
    "print(X_train_scaled_poly.shape)\n",
    "print(X_test_scaled_poly.shape)\n",
    "\n",
    "# PolynomialFeatures(\n",
    "    # degree — степень полинома. По умолчанию используется степень 2\n",
    "    # include_bias — включать ли в результирующую таблицу столбец из единиц (x в степени 0). \n",
    "        # По умолчанию стоит True, но лучше выставить его в значение False, так как столбец из единиц и \n",
    "        # так добавляется в методе наименьших квадратов.\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### РЕГУЛЯРИЗАЦИЯ (борьба с разбросом (переобучением)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Недообучение (underfitting) — проблема, обратная переобучению. Модель из-за своей слабости не уловила никаких закономерностей в данных. В этом случае ошибка будет высокой как для тренировочных данных, так и для данных, не показанных во время обучения.\n",
    "\n",
    "Смещение (bias) — это математическое ожидание разности между истинным ответом и ответом, выданным моделью. То есть это ожидаемая ошибка модели.\n",
    "\n",
    "Разброс (variance) — это вариативность ошибки, то, насколько ошибка будет отличаться, если обучать модель на разных наборах данных. Математически это дисперсия (разброс) ответов модели.\n",
    "\n",
    "Регуляризация — способ уменьшения переобучения моделей машинного обучения.\n",
    "\n",
    "Штраф — это дополнительное неотрицательное слагаемое в выражении для функции потерь, которое специально повышает ошибку.  За счёт этого слагаемого метод оптимизации (OLS или SGD) будет находить не истинный минимум функции потерь, а псевдоминимум."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1-регуляризация (Lasso) \n",
    "# Создаём объект класса линейной регрессии с L1-регуляризацией\n",
    "lasso_lr_poly = linear_model.Lasso(alpha=0.1)\n",
    "\n",
    "# Обучаем модель\n",
    "lasso_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "\n",
    "# Делаем предсказание для тренировочной выборки\n",
    "y_train_predict_poly = lasso_lr_poly.predict(X_train_scaled_poly)\n",
    "# Делаем предсказание для тестовой выборки\n",
    "y_test_predict_poly = lasso_lr_poly.predict(X_test_scaled_poly)\n",
    "\n",
    "# Рассчитываем коэффициент детерминации для двух выборок\n",
    "print(\"Train R^2: {:.3f}\".format(metrics.r2_score(y_train, y_train_predict_poly)))\n",
    "print(\"Test R^2: {:.3f}\".format(metrics.r2_score(y_test, y_test_predict_poly)))\n",
    "\n",
    "# Lasso(\n",
    "    # Главный параметр инициализации Lasso — это alpha, коэффициент регуляризации. \n",
    "        # По умолчанию alpha=1. Практика показывает, что это довольно сильная регуляризация для L1-метода. \n",
    "        # Давайте установим значение этого параметра на 0.1.\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2-регуляризация (Ridge)\n",
    "# Создаём объект класса линейной регрессии с L2-регуляризацией\n",
    "ridge_lr_poly = linear_model.Ridge(alpha=10)\n",
    "\n",
    "# Обучаем модель\n",
    "ridge_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "\n",
    "# Делаем предсказание для тренировочной выборки\n",
    "y_train_predict_poly = ridge_lr_poly.predict(X_train_scaled_poly)\n",
    "# Делаем предсказание для тестовой выборки\n",
    "y_test_predict_poly = ridge_lr_poly.predict(X_test_scaled_poly)\n",
    "\n",
    "# Рассчитываем коэффициент детерминации для двух выборок\n",
    "print(\"Train R^2: {:.3f}\".format(metrics.r2_score(y_train, y_train_predict_poly)))\n",
    "print(\"Test R^2: {:.3f}\".format(metrics.r2_score(y_test, y_test_predict_poly)))\n",
    "\n",
    "# Ridge(\n",
    "    # Главный параметр инициализации Lasso — это alpha, коэффициент регуляризации. \n",
    "        # Для L2-регуляризации параметр alpha по умолчанию равен 1. \n",
    "        # Давайте попробуем использовать значение параметра alpha=10:\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ЛУЧШЕ ТА, У КОТОРОЙ ВЫШЕ ПОКАЗАТЕЛИ НА ТЕСТОВОЙ ВЫБОРКЕ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр alpha имеет очень важное значение: от его выбора зависит, как сильно мы будем штрафовать модель за переобучение. Важно найти значение, которое приносит наилучший эффект.\n",
    "\n",
    "Попробуйте вручную изменять параметр alpha для построенных ранее моделей. Согласитесь, это не очень удобно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём список из 20 возможных значений от 0.001 до 1\n",
    "alpha_list = np.linspace(0.001, 1, 20)\n",
    "# Создаём пустые списки, в которые будем добавлять результаты \n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for alpha in alpha_list:\n",
    "    # Создаём объект класса линейной регрессии с L1-регуляризацией\n",
    "    lasso_lr_poly = linear_model.Lasso(alpha=alpha, max_iter=10000)\n",
    "    \n",
    "    # Обучаем модель\n",
    "    lasso_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "    \n",
    "    # Делаем предсказание для тренировочной выборки\n",
    "    y_train_predict_poly = lasso_lr_poly.predict(X_train_scaled_poly)\n",
    "    # Делаем предсказание для тестовой выборки\n",
    "    y_test_predict_poly = lasso_lr_poly.predict(X_test_scaled_poly)\n",
    "    \n",
    "    # Рассчитываем коэффициенты детерминации для двух выборок и добавляем их в списки\n",
    "    train_scores.append(metrics.r2_score(y_train, y_train_predict_poly))\n",
    "    test_scores.append(metrics.r2_score(y_test, y_test_predict_poly))\n",
    "\n",
    "# Визуализируем изменение R^2 в зависимости от alpha\n",
    "fig, ax = plt.subplots(figsize=(12, 4)) # фигура + координатная плоскость\n",
    "ax.plot(alpha_list, train_scores, label='Train') # линейный график для тренировочной выборки\n",
    "ax.plot(alpha_list, test_scores, label='Test') # линейный график для тестовой выборки\n",
    "ax.set_xlabel('Alpha') # название оси абсцисс\n",
    "ax.set_ylabel('R^2') # название оси ординат\n",
    "ax.set_xticks(alpha_list) # метки по оси абсцисс\n",
    "ax.xaxis.set_tick_params(rotation=45) # поворот меток на оси абсцисс\n",
    "ax.legend(); # отображение легенды   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбираем на графике лучший результат *alpha*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём объект класса линейной регрессии с L1-регуляризацией\n",
    "lasso_lr_poly = linear_model.Lasso(alpha=0.0536)\n",
    "\n",
    "# Обучаем модель \n",
    "lasso_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "\n",
    "# Делаем предсказание для тренировочной выборки\n",
    "y_train_predict_poly = lasso_lr_poly.predict(X_train_scaled_poly)\n",
    "# Делаем предсказание для тестовой выборки\n",
    "y_test_predict_poly = lasso_lr_poly.predict(X_test_scaled_poly)\n",
    "\n",
    "# Рассчитываем коэффициент детерминации для двух выборок\n",
    "print(\"Train R^2: {:.3f}\".format(metrics.r2_score(y_train, y_train_predict_poly)))\n",
    "print(\"Test R^2: {:.3f}\".format(metrics.r2_score(y_test, y_test_predict_poly)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### МЕТРИКИ РЕГРЕССИИ \n",
    "``` python\n",
    "    from sklearn import metrics\n",
    "```\n",
    "* MAE (Mean Absolute Error)             - mean_absolute_error(y, y_pred)\n",
    "* MAPE (Mean Absolute Percent Error)    - mean_absolute_percentage_error(y, y_pred)*100\n",
    "* MSE (Mean Square Error)               - mean_square_error(y, y_pred)\n",
    "* RMSE (Root Mean Squared Error)        - np.sqrt(mean_square_error(y, y_pred))\n",
    "* R2 (Коэффициент детерминации)         - r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Делаем предсказание по всем признакам\n",
    "y_predict = model.predict(X)\n",
    "# Рассчитываем MAE - Средняя абсолютная ошибка\n",
    "print('MAE score: {:.3f} thou. $'.format(metrics.mean_absolute_error(y, y_predict)))\n",
    "# Рассчитываем MAPE - Средняя абсолютная ошибка в процентах\n",
    "print('MAPE score: {:.3f} %'.format(metrics.mean_absolute_percentage_error(y, y_predict) * 100))\n",
    "# Рассчитываем RMSE - Корень из средней квадратической ошибки\n",
    "print('RMSE score: {:.3f} thou. $'.format(np.sqrt(metrics.mean_squared_error(y, y_predict))))\n",
    "# Рассчитываем коэффициент детерминации Коэффициент детерминации (R2)\n",
    "print('R2 score: {:.3f}'.format(metrics.r2_score(y, y_predict)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***КЛАССИФИКАЦИЯ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ЧИСЛОВОЕ РЕШЕНИЕ \"Логистическая регрессия\"\n",
    "``` python\n",
    "    from sklearn import linear_model\n",
    "    log_reg_model = linear_model.LogisticRegression()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### МУЛЬТИКЛАССОВАЯ КЛАССИФИКАЦИЯ (когда не 2 класса, а больше)\n",
    "* Сравнивает класс 0 с классом 1 и 2, потом класс 1 с классами 0 и 2 и т. д.\n",
    "``` python\n",
    "    from sklearn import linear_model\n",
    "    log_reg_model = linear_model.LogisticRegression(\n",
    "        multi_class='multinomial', #мультиклассовая классификация\n",
    "    )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ДЕРЕВЬЯ РЕШЕНИЙ (связанный ациклический граф)\n",
    "* состоит из корневой вершины, внутренних вершин и листьев\n",
    "* Предикаты - критерии находящиеся на вершинах\n",
    "``` python\n",
    "    from sklearn import tree # Модели деревьев решения\n",
    "    # Создаём объект класса DecisionTreeClassifier\n",
    "    dt_clf_model = tree.DecisionTreeClassifier()\n",
    "    dt_clf_model.get_depth() # Показывает дерево\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### АНСАМБЛИ (БЕГГИНГ) \"Случайный лес\"\n",
    "* Много слабых моделей объединяются в одну\n",
    "* Виды ансамблей \n",
    "    * Бэггинг — параллельно обучаем множество одинаковых моделей, а для предсказания берём среднее по предсказаниям каждой из моделей.\n",
    "    * Бустинг — последовательно обучаем множество одинаковых моделей, где каждая новая модель концентрируется на тех примерах, где предыдущая допустила ошибку.\n",
    "    * Стекинг — параллельно обучаем множество разных моделей, отправляем их результаты в финальную модель, и уже она принимает решение.\n",
    "``` python \n",
    "    from sklearn import ensemble\n",
    "    rf_clf_model = ensemble.RandomForestClassifier(\n",
    "        n_estimators=500, #число деревьев\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### МЕТРИКИ КЛАССИФИКАЦИИ \n",
    "```python\n",
    "    from sklearn import metrics\n",
    "```\n",
    "* Матрица ошибок                        - confusion_matrix(y, y_pred)\n",
    "* Accuracy (достоверность)              - accuracy_score(y, y_pred)\n",
    "    * чем ближе к 1, тем больше угадала\n",
    "* Precision (точность) или              - precision_score(y, y_pred)\n",
    "    * PPV (Positive Predictive Value)\n",
    "    * чем ближе к 1, тем меньше ошибок\n",
    "* Recall (полнота) или                  - recall_score(y, y_pred)\n",
    "    * TPR (True Positive Rate)\n",
    "    * чем ближе к 1, тем больше значений 1 класса названо правильно\n",
    "* F-мера                                - f1_score(y, y_pred)\n",
    "    * (это взвешенное среднее гармоническое между precision и recall)\n",
    "    * чем ближе к 1, тем точнее модель\n",
    "* Все ошибки в одном отчете             - classification_report(y, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***КЛАСТЕРИЗАЦИЯ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ОПРЕДЕЛЕНИЕ ОПТИМАЛЬНОГО КОЛИЧЕСТВА КЛАСТЕРОВ\n",
    "* #### МЕТОД ЛОКТЯ - построение графика зависимости инерции от количества кластеров\n",
    "    ``` python\n",
    "        model.inertia_ # получение инерции для создания графика\n",
    "    ```\n",
    "\n",
    "* #### КОЭФФИЦИЕНТ СИЛУЭТА - построение графика зависимости коэффициента силуэта от количества кластеров\n",
    "    * Коэффициент силуэта показывает, насколько объект похож на объекты кластера, в котором он находится, по сравнению с объектами из других кластеров.\n",
    "    ``` python\n",
    "    silhouette_score(X, model.labels_)\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM - АЛГОРИТМЫ КЛАСТЕРИЗАЦИИ\n",
    "* В основе данного подхода лежит предположение, что любой объект принадлежит ко всем кластерам, но с разной вероятностью.\n",
    "* Представителями являются:\n",
    "    * K-means кластеризация - например, для кластеризации документов\n",
    "    * GMM кластеризация - например, для сегментации изображения\n",
    "\n",
    "\n",
    "### КЛАСТЕРИЗАЦИЯ \"АЛГОРИТМ K-MEANS\"\n",
    "* Чувствительна к выбросам\n",
    "* K-MEANS - центроиды кластера это средние значения\n",
    "* K-MEANS++ - центроиды кластера это средние значения, но первые значения выбираются не случайно\n",
    "* K-MEDIANS - центроиды кластера это медианные значения\n",
    "* K-MEDOIDS - центроиды кластера это медианные значения, но не расчетное значение, а ближайшая к нему точка\n",
    "* FUZZY C-MEANS - каждый объект может принадлежать к разным кластерам с разной вероятностью\n",
    "``` python\n",
    "    from sklearn.cluster import KMeans\n",
    "    k_means_model = KMeans(n_clusters=2, init='k-means++', n_init=10, random_state=42)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### КЛАСТЕРИЗАЦИЯ \"GMM\" - модель гауссовой смеси (Gaussian Mixture Model)\n",
    "* Чувствительна к выбросам\n",
    "``` python\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    gm_clst_model = GaussianMixture()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"ИЕРАРХИЧЕСКАЯ\" КЛАСТЕРИЗАЦИЯ\n",
    "* Принцип иерархической кластеризации основан на построении дерева (иерархии) вложенных кластеров.\n",
    "* Строится дендрограмма - это древовидная диаграмма, которая содержит  уровней. Каждый уровень — это шаг укрупнения кластеров.\n",
    "* Методы построения дендрограммы:\n",
    "    * Агломеративный метод (agglomerative) - объединение мелких кластеров\n",
    "    * Дивизионный (дивизивный) метод (divisive) - деление крупных кластеров\n",
    "* Методы расчета расстояния между кластерами\n",
    "    * Метод одиночной связи (min расстояние)\n",
    "    * Метод полной связи    (max расстояние)\n",
    "    * Метод средней связи   (mean расстояние)\n",
    "    * Центроидный метод     (расстояние м/ж центрами)\n",
    "``` python\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    agg_clst_model = AgglomerativeClustering()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ПОНИЖЕНИЕ РАЗМЕРНОСТИ**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### СПЕКТРАЛЬНАЯ КЛАСТЕРИЗАЦИЯ\n",
    "* \n",
    "* Применяется для: \n",
    "* сегментации изображений\n",
    "``` python\n",
    "    from sklearn.cluster import SpectralClustering\n",
    "    spectral_clst_model = SpectralClustering()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### PCA - линейное преобразование\n",
    "* Метод главных компонент, или PCA (Principal Components Analysis)\n",
    "* это один из базовых способов уменьшения размерности\n",
    "* Применяется для:  \n",
    "    * подавление шума\n",
    "    * индексация видео\n",
    "``` python\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE - нелинейное преобразование\n",
    "* t-SNE (t-distributed Stochastic Neighbor Embedding)\n",
    "* «стохастическое вложение соседей с t-распределением»\n",
    "* при преобразовании похожие объекты оказываются рядом, а непохожие — далеко друг от друга\n",
    "* Применяется для:  \n",
    "    * уменьшения размерность до 2х или 3х мерного\n",
    "``` python\n",
    "    from sklearn.manifold import TSNE\n",
    "    tsne = TSNE()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### КЛАСТЕРИЗАЦИЯ НА ОСНОВЕПЛОТНОСТИ (DBSCAN)\n",
    "* DENSITY-BASED SPATIAL CLUSTERING OF APPLICATIONS WITH NOISE\n",
    "* В отличие от k-means, не нужно задавать количество кластеров — алгоритм сам определит оптимальное\n",
    "* Алгоритм хорошо работает с данными произвольной формы\n",
    "* DBSCAN отлично справляется с выбросами в датасетах\n",
    "``` python\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    clst_model = DBSCAN()    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ВИЗУАЛИЗАЦИЯ КЛАСТЕРИЗАЦИЙ\n",
    "* диаграмма рассеяния для двухмерного и трёхмерного случаев \n",
    "* Convex Hull, или выпуклая оболочка - провести гарницы\n",
    "* дендрограмма \n",
    "* Clustergram - \"полосы\" - только для иерархических кластеризаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### МЕТРИКИ КЛАСТЕРИЗАЦИИ \n",
    "``` python\n",
    "    from sklearn.metrics.cluster import homogeneity_score\n",
    "    from sklearn.metrics.cluster import completeness_score\n",
    "    from sklearn.metrics.cluster import v_measure_score\n",
    "    from sklearn.metrics.cluster import rand_score\n",
    "```\n",
    "* Однородность кластеров                - homogeneity_score(y, y_pred)\n",
    "* Полнота кластеров                     - completeness_score(y, y_pred)\n",
    "* V-мера (комбинация однор. и полн.)    - v_measure_score(y, y_pred)\n",
    "* Индекс Ренда                          - rand_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# МЕТРИКИ КЛАССИФИКАЦИИ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy** (достоверность) — доля правильных ответов модели среди всех ответов. \n",
    "* Интерпретация: как много (в долях) модель угадала ответов.\n",
    "* Accuracy — самая простая и самая понятная метрика классификации, но у неё есть один существенный недостаток. \n",
    "* Она бесполезна, если классы сильно несбалансированы.\n",
    "\n",
    "**Precision** (точность), или PPV (Positive Predictive Value) — это доля объектов, которые действительно являются положительными, по отношению ко всем объектам, названным моделью положительными.\n",
    "* Интерпретация: способность отделить класс 1 от класса 0. Чем больше precision, тем меньше ложных попаданий. \n",
    "* Precision нужен в задачах, где от нас требуется минимум ложных срабатываний. Чем выше «цена» ложноположительного результата, тем выше должен быть precision.\n",
    "* Можно использовать на несбалансированных выборках.\n",
    "\n",
    "**Recall** (полнота), или TPR (True Positive Rate) — это доля объектов, названных классификатором положительными, по отношению ко всем объектам положительного класса.\n",
    "* Интерпретация: способность модели обнаруживать класс 1 вообще, то есть охват класса 1. Заметьте, что ложные срабатывания не влияют на recall. \n",
    "* Recall очень хорошо себя показывает в задачах, где важно найти как можно больше объектов, принадлежащих к классу 1.\n",
    "* Можно использовать на несбалансированных выборках.\n",
    "\n",
    "\n",
    "Концентрация только на одной метрике (precision или recall) без учёта второй — сомнительная идея.\n",
    "В битве за максимум precision для класса 1 побеждает модель, которая всегда будет говорить говорить «нет». У неё вообще не будет ложных срабатываний.\n",
    "В битве за максимум recall для класса 1 побеждает модель, которая всегда будет говорить «да». Она охватит все наблюдения класса 1. \n",
    "В реальности необходимо балансировать между двумя этими метриками.\n",
    "\n",
    "**F1** (F-мера) — это взвешенное среднее гармоническое между precision и recall:\n",
    "* Несмотря на отсутствие бизнес-интерпретации, метрика F1 является довольно распространённой и используется в задачах, где необходимо выбрать модель, которая балансирует между precision и recall.\n",
    "* Используется в задачах, где необходимо балансировать между precision и recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Модель log_reg_full:\n",
    "#Рассчитываем accuracy\n",
    "print('Accuracy: {:.2f}'.format(metrics.accuracy_score(y, y_pred)))\n",
    "#Рассчитываем precision\n",
    "print('Precision: {:.2f}'.format(metrics.precision_score(y, y_pred)))\n",
    "#Рассчитываем recall\n",
    "print('Recall: {:.2f}'.format(metrics.recall_score(y, y_pred)))\n",
    "#Рассчитываем F1-меру\n",
    "print('F1 score: {:.2f}'.format(metrics.f1_score(y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метрики классификации в одной строке\n",
    "print(metrics.classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(model, X, y, cv, scoring=\"f1\", ax=None, title=\"\"):\n",
    "    # Вычисляем координаты для построения кривой обучения\n",
    "    train_sizes, train_scores, valid_scores = model_selection.learning_curve(\n",
    "        estimator=model,  # модель\n",
    "        X=X,  # матрица наблюдений X\n",
    "        y=y,  # вектор ответов y\n",
    "        cv=cv,  # кросс-валидатор\n",
    "        scoring=scoring,  # метрика\n",
    "    )\n",
    "    # Вычисляем среднее значение по фолдам для каждого набора данных\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    valid_scores_mean = np.mean(valid_scores, axis=1)\n",
    "    # Если координатной плоскости не было передано, создаём новую\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))  # фигура + координатная плоскость\n",
    "    # Строим кривую обучения по метрикам на тренировочных фолдах\n",
    "    ax.plot(train_sizes, train_scores_mean, label=\"Train\")\n",
    "    # Строим кривую обучения по метрикам на валидационных фолдах\n",
    "    ax.plot(train_sizes, valid_scores_mean, label=\"Valid\")\n",
    "    # Даём название графику и подписи осям\n",
    "    ax.set_title(\"Learning curve: {}\".format(title))\n",
    "    ax.set_xlabel(\"Train data size\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    # Устанавливаем отметки по оси абсцисс\n",
    "    ax.xaxis.set_ticks(train_sizes)\n",
    "    # Устанавливаем диапазон оси ординат\n",
    "    ax.set_ylim(0, 1)\n",
    "    # Отображаем легенду\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что отображено в print(metrics.classification_report(y, y_pred))?\n",
    "\n",
    "1) В первой части таблицы отображаются метрики precision, recall и f1-score, рассчитанные для каждого класса в отдельности. Столбец support — это количество объектов каждого из классов.\n",
    "2) Во второй части таблицы отображена общая метрика accuracy. \n",
    "3) Далее идёт строка macro avg — это среднее значение метрики между классами 1 и 0. Например, значение в строке macro avg и столбце recall = (0.88 + 0.56)/2=0.72.\n",
    "4) Завершает отчёт строка weighted avg — это средневзвешенное значение метрики между классами 1 и 0. Рассчитывается по формуле:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нас интересует только вероятность класса (второй столбец)\n",
    "y_test_proba_pred = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "# Для удобства завернем numpy-массив в pandas Series\n",
    "y_test_proba_pred = pd.Series(y_test_proba_pred)\n",
    "# Создадим списки, в которых будем хранить значения метрик \n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "f1_scores = []\n",
    "# Сгенерируем набор вероятностных порогов в диапазоне от 0.1 до 1\n",
    "thresholds = np.arange(0.1, 1, 0.05)\n",
    "# В цикле будем перебирать сгенерированные пороги\n",
    "for threshold in thresholds:\n",
    "    # Пациентов, для которых вероятность наличия диабета > threshold относим к классу 1\n",
    "    # В противном случае - к классу 0\n",
    "    y_test_pred = y_test_proba_pred.apply(lambda x: 1 if x>threshold else 0)\n",
    "    # Считаем метрики и добавляем их в списки\n",
    "    recall_scores.append(metrics.recall_score(y_test, y_test_pred))\n",
    "    precision_scores.append(metrics.precision_score(y_test, y_test_pred))\n",
    "    f1_scores.append(metrics.f1_score(y_test, y_test_pred))\n",
    "\n",
    "# Визуализируем метрики при различных threshold\n",
    "fig, ax = plt.subplots(figsize=(10, 4)) #фигура + координатная плоскость\n",
    "# Строим линейный график зависимости recall от threshold\n",
    "ax.plot(thresholds, recall_scores, label='Recall')\n",
    "# Строим линейный график зависимости precision от threshold\n",
    "ax.plot(thresholds, precision_scores, label='Precision')\n",
    "\n",
    "# Строим линейный график зависимости F1 от threshold\n",
    "ax.plot(thresholds, f1_scores, label='F1-score')\n",
    "# Даем графику название и подписи осям\n",
    "ax.set_title('Recall/Precision dependence on the threshold')\n",
    "ax.set_xlabel('Probability threshold')\n",
    "ax.set_ylabel('Score')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задаем оптимальный порог вероятностей\n",
    "threshold_opt = 0.4 # полученное по графику значение\n",
    "# Людей, у которых вероятность зарабатывать >50K больше 0.5 относим к классу 1\n",
    "# В противном случае - к классу 0\n",
    "y_test_pred_opt = y_test_proba_pred.apply(lambda x: 1 if x > threshold_opt else 0)\n",
    "# Считаем метрики\n",
    "print(metrics.classification_report(y_test, y_test_pred_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ В \"КЛАССИФИКАЦИИ\" С ПОМОЩЬЮ SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем объект класса логистическая регрессия\n",
    "log_reg_full = linear_model.LogisticRegression(random_state=42, max_iter=1000)\n",
    "# random_state — число, на основе которого происходит генерация случайных чисел.\n",
    "\n",
    "# multi_class='multinomial' - мультиклассовая классификация\n",
    "\n",
    "# penalty — метод регуляризации. Возможные значения:\n",
    "#   'l1' — L1-регуляризация;\n",
    "#   'l2' — L2-регуляризация (используется по умолчанию);\n",
    "#   'elasticnet' — эластичная сетка (L1+L2);\n",
    "#   'none' — отсутствие регуляризации.\n",
    "\n",
    "# C — коэффициент обратный коэффициенту регуляризации, то есть равен C/α. \n",
    "# Чем больше C, тем меньше регуляризация. По умолчанию C=1, тогда α=1.\n",
    "\n",
    "# solver — численный метод оптимизации функции потерь logloss, может быть:\n",
    "#   'sag' — стохастический градиентный спуск (нужна стандартизация/нормализация);\n",
    "#   'saga' — модификация предыдущего, которая поддерживает работу с негладкими функциями (нужна стандартизация/нормализация);\n",
    "#   'newton-cg' — метод Ньютона с модификацией сопряжённых градиентов (не нужна стандартизация/нормализация);\n",
    "#   'lbfgs' — метод Бройдена — Флетчера — Гольдфарба — Шанно (не нужна стандартизация/нормализация; используется по умолчанию, так как из всех методов теоретически обеспечивает наилучшую сходимость);\n",
    "#   'liblinear' — метод покоординатного спуска (не нужна стандартизация/нормализация).\n",
    "\n",
    "# max_iter — максимальное количество итераций, выделенных на сходимость.\n",
    "\n",
    "# class_weight='balanced' - сбалансированность весов классов для дисбалансных классификаций\n",
    "\n",
    "# Обучаем модель, минизируя logloss\n",
    "log_reg_full.fit(X, y)\n",
    "\n",
    "# Делаем предсказание класса\n",
    "y_pred = log_reg_full.predict(X)\n",
    "\n",
    "# Выводим результирующие коэффициенты\n",
    "print('w0: {}'.format(log_reg_full.intercept_)) #свободный член w0\n",
    "print('w1, w2: {}'.format(log_reg_full.coef_)) #остальные параметры модели w1, w2, ..., wm\n",
    "\n",
    "# Значения концентации глюкозы и индекса массы тела для пациента\n",
    "x_new = [[180, 51]]\n",
    "\n",
    "# Делаем предсказание класса:\n",
    "y_new_predict = log_reg_full.predict(x_new)\n",
    "print('Predicted class: {}'.format(y_new_predict))\n",
    "\n",
    "# Делаем предсказание вероятностей:\n",
    "y_new_proba_predict = log_reg_full.predict_proba(x_new)\n",
    "print('Predicted probabilities: {}'.format(np.round(y_new_proba_predict, 2)))\n",
    "\n",
    "# Создадим временную таблицу X\n",
    "X_temp = X.copy()\n",
    "# Добавим в эту таблицу результат предсказания\n",
    "X_temp['Prediction'] = y_pred\n",
    "X_temp.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЕРЕВЬЯ РЕШЕНИЙ В \"КЛАССИФИКАЦИИ\" С ПОМОЩЬЮ SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree # модели деревьев решения\n",
    "\n",
    "# Создаем объект класса дерево решений\n",
    "dt = tree.DecisionTreeClassifier(\n",
    "    criterion='entropy',\n",
    "    min_samples_leaf=5,\n",
    "    max_depth=8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# criterion — критерий информативности ('gini' — критерий Джини и 'entropy' — энтропия Шеннона).\n",
    "\n",
    "# max_depth — максимальная глубина дерева (по умолчанию — None, глубина дерева не ограничена).\n",
    "\n",
    "# max_features — максимальное число признаков, по которым ищется лучшее разбиение в дереве \n",
    "# (по умолчанию — None, то есть обучение производится на всех признаках). Нужно потому, \n",
    "# что при большом количестве признаков будет «дорого» искать лучшее (по критерию типа прироста информации) \n",
    "# разбиение среди всех признаков.\n",
    "\n",
    "# min_samples_leaf — минимальное число объектов в листе (по умолчанию — 1). \n",
    "# У этого параметра есть понятная интерпретация: если он равен 5, то дерево будет порождать \n",
    "# только те решающие правила, которые верны как минимум для пяти объектов.\n",
    "\n",
    "# random_state — число, отвечающее за генерацию случайных чисел.\n",
    "\n",
    "# Обучаем дерево по алгоритму CART\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Выводим значения метрики \n",
    "y_train_pred = dt.predict(X_train)\n",
    "print('Train: {:.2f}'.format(metrics.f1_score(y_train, y_train_pred)))\n",
    "y_test_pred = dt.predict(X_test)\n",
    "print('Test: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение дерева на графике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем фигуру для визуализации графа\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "# Строим граф дерева решений\n",
    "tree_graph = tree.plot_tree(\n",
    "    dt, # объект обученного дерева\n",
    "    feature_names=X_train.columns, # наименования факторов\n",
    "    class_names=[\"0 - <=50K\", \"1 - >50K\"], # имена классов\n",
    "    filled=True, # расцветка графа\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение графика с областями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probabilities_2d(X, y, model):\n",
    "    # Генерируем координатную сетку из всех возможных значений для признаков\n",
    "    # Glucose изменяется от 40 до 200, BMI — от 10 до 80\n",
    "    # Результат работы функции — два массива xx1 и xx2, которые образуют координатную сетку\n",
    "    xx1, xx2 = np.meshgrid(\n",
    "        np.arange(40, 200, 0.1),\n",
    "        np.arange(10, 80, 0.1)\n",
    "    )\n",
    "    # Вытягиваем каждый из массивов в вектор-столбец — reshape(-1, 1)\n",
    "    # Объединяем два столбца в таблицу с помощью hstack\n",
    "    X_net = np.hstack([xx1.reshape(-1, 1), xx2.reshape(-1, 1)])\n",
    "    # Предсказываем вероятность для всех точек на координатной сетке\n",
    "    # Нам нужна только вероятность класса 1\n",
    "    probs = model.predict_proba(X_net)[:, 1]\n",
    "    # Переводим столбец из вероятностей в размер координатной сетки\n",
    "    probs = probs.reshape(xx1.shape)\n",
    "    # Создаём фигуру и координатную плоскость\n",
    "    fig, ax = plt.subplots(figsize = (10, 5))\n",
    "    # Рисуем тепловую карту вероятностей\n",
    "    contour = ax.contourf(xx1, xx2, probs, 100, cmap='bwr')\n",
    "    # Рисуем разделяющую плоскость — линию, где вероятность равна 0.5\n",
    "    bound = ax.contour(xx1, xx2, probs, [0.5], linewidths=2, colors='black');\n",
    "    # Добавляем цветовую панель \n",
    "    colorbar = fig.colorbar(contour)\n",
    "    # Накладываем поверх тепловой карты диаграмму рассеяния\n",
    "    sns.scatterplot(data=X, x='Glucose', y='BMI', hue=y, palette='seismic', ax=ax)\n",
    "    # Даём графику название\n",
    "    ax.set_title('Scatter Plot with Decision Boundary');\n",
    "    # Смещаем легенду в верхний левый угол вне графика\n",
    "    ax.legend(bbox_to_anchor=(-0.05, 1))\n",
    "\n",
    "# Вызовем нашу функцию для визуализации:\n",
    "plot_probabilities_2d(X, y, dt_clf_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важность признаков можно посмотреть, обратившись к атрибуту feature_importance_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt_clf_full.feature_importances_)\n",
    "\n",
    "# А лучше через столбчатую диаграмму\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 5)) #фигура + координатная плоскость\n",
    "feature = X.columns #признаки\n",
    "feature_importances = dt_clf_full.feature_importances_ #важность признаков\n",
    "# Строим столбчатую диаграмму\n",
    "sns.barplot(x=feature, y=feature_importances, ax=ax);\n",
    "# Добавляем подпись графику, осям абсцисс и ординат\n",
    "ax.set_title('Bar plot feature importances')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_ylabel('Importances');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# АНСАМБЛИ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ансамблевые модели или просто ансамбли (ensembles) — это метод машинного обучения, где несколько простых моделей (часто называемых «слабыми учениками») обучаются для решения одной и той же задачи и объединяются для получения лучших результатов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует три проверенных способа построения ансамблей:\n",
    "\n",
    "* Бэггинг — параллельно обучаем множество одинаковых моделей, а для предсказания берём среднее по предсказаниям каждой из моделей.\n",
    "* Бустинг — последовательно обучаем множество одинаковых моделей, где каждая новая модель концентрируется на тех примерах, где предыдущая допустила ошибку.\n",
    "* Стекинг — параллельно обучаем множество разных моделей, отправляем их результаты в финальную модель, и уже она принимает решение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble #ансамбли"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# СЛУЧАЙНЫЙ ЛЕС"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Случайный лес (Random Forest)** - частный случай бэггинга над деревьями решений с методом случайных подпространств"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём объект класса RandomForestClassifier\n",
    "rf = ensemble.RandomForestClassifier(\n",
    "    n_estimators=500, # число деревьев\n",
    "    criterion='entropy', # критерий эффективности\n",
    "    max_depth=3, # максимальная глубина дерева\n",
    "    min_samples_leaf=10, # минимальное число объектов в листе\n",
    "    max_features='sqrt', # число признаков из метода случайных подпространств\n",
    "    random_state=42 # генератор случайных чисел\n",
    ")\n",
    "\n",
    "# n_estimators — количество деревьев в лесу (число K из бэггинга; по умолчанию равно 100);\n",
    "\n",
    "# criterion — критерий информативности разбиения для каждого из деревьев \n",
    "# ('gini' — критерий Джини и 'entropy' — энтропия Шеннона; по умолчанию — 'gini');\n",
    "\n",
    "# max_depth — максимальная глубина одного дерева (по умолчанию — None, то есть глубина дерева не ограничена);\n",
    "\n",
    "# max_features — максимальное число признаков, которые будут использоваться каждым из деревьев \n",
    "# (число L из метода случайных подпространств; по умолчанию — 'sqrt'; \n",
    "# для обучения каждого из деревьев используется корень из m признаков, \n",
    "# где m — число признаков в начальном наборе данных);\n",
    "\n",
    "# min_samples_leaf — минимальное число объектов в листе (по умолчанию — 1);\n",
    "\n",
    "# random_state — параметр, отвечающий за генерацию случайных чисел.\n",
    "\n",
    "# Обучаем модель \n",
    "rf.fit(X_train, y_train)\n",
    " \n",
    "#Выводим значения метрики \n",
    "y_train_pred = rf.predict(X_train)\n",
    "print('Train: {:.2f}'.format(metrics.f1_score(y_train, y_train_pred)))\n",
    "y_test_pred = rf.predict(X_test)\n",
    "print('Test: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вызовем нашу функцию для визуализации:\n",
    "plot_probabilities_2d(X, y, rf)\n",
    "\n",
    "# Также можно показать важность признаков через столбчатую диаграмму"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_proba_pred = pd.Series(rf.predict_proba(X_test)[:, 1])\n",
    "#Создадим списки, в которых будем хранить значения метрик \n",
    "f1_scores = []\n",
    "#Сгенерируем набор вероятностных порогов в диапазоне от 0.1 до 1\n",
    "thresholds = np.arange(0.1, 1, 0.05)\n",
    "#В цикле будем перебирать сгенерированные пороги\n",
    "for threshold in thresholds:\n",
    "    y_test_pred_poly = y_test_proba_pred.apply(lambda x: 1 if x > threshold else 0)\n",
    "    #Считаем метрики и добавляем их в списки\n",
    "    f1_scores.append(metrics.f1_score(y_test, y_test_pred_poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Визуализируем метрики при различных threshold\n",
    "fig, ax = plt.subplots(figsize=(10, 4)) #фигура + координатная плоскость\n",
    "#Строим линейный график зависимости F1 от threshold\n",
    "ax.plot(thresholds, f1_scores, label='F1-score')\n",
    "\n",
    "#Даем графику название и подписи осям\n",
    "ax.set_title('F1 dependence on the threshold')\n",
    "ax.set_xlabel('Probability threshold')\n",
    "ax.set_ylabel('Score')\n",
    "#Устанавливаем отметки по оси x\n",
    "ax.set_xticks(thresholds) \n",
    "# Подписываем названия осей\n",
    "ax.set_xlabel('Probability threshold')\n",
    "ax.set_ylabel('Score')\n",
    "#Отображаем легенду\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# КЛАСТЕРИЗАЦИЯ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# МЕТРИКИ КЛАСТЕРИЗАЦИИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем подсчет метрики ОДНОРОДНОСИТ кластеров\n",
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "\n",
    "# передаем предсказанную информацию к какому кластеру относятся объекты датасета и правильные ответы\n",
    "print(homogeneity_score(labels_true=[0, 0, 1, 1], labels_pred=[0, 0, 1, 1]))\n",
    "\n",
    "# теперь посчитаем насколько однородными получились кластеры с покемонами\n",
    "print(homogeneity_score(labels_true=df.RealClusters, labels_pred=df.Clusters_k3))\n",
    "\n",
    "print(homogeneity_score(labels_true=df.RealClusters, labels_pred=df.Clusters_k4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем подсчет метрики ПОЛНОТЫ кластеров\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "\n",
    "# передаем предсказанную информацию к какому кластеру относятся объекты датасета и правильные ответы, подсчитываем метрику\n",
    "print(completeness_score(labels_true=[0, 0, 1, 1], labels_pred=[0, 0, 1, 1]))\n",
    "\n",
    "# посчитаем насколько полными получились кластеры с покемонами\n",
    "print(completeness_score(labels_true=df.RealClusters, labels_pred=df.Clusters_k3))\n",
    "\n",
    "# посчитаем насколько полными получились кластеры с покемонами\n",
    "print(completeness_score(labels_true=df.RealClusters, labels_pred=df.Clusters_k4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем из библиотеки sklearn подсчет V-меры\n",
    "from sklearn.metrics import v_measure_score\n",
    "# Эта метрика — комбинация метрик полноты и однородности.\n",
    "\n",
    "# теперь посчитаем v-меру для кластеров с покемонами\n",
    "print(v_measure_score(labels_true=df.RealClusters, labels_pred=df.Clusters_k3))\n",
    "\n",
    "print(v_measure_score(labels_true=df.RealClusters, labels_pred=df.Clusters_k4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем из библиотеки sklearn подсчет ИНДЕКСА РЭНДА\n",
    "from sklearn.metrics.cluster import rand_score\n",
    "\n",
    "# теперь посчитаем насколько полными получились кластеры с покемонами\n",
    "print(rand_score(labels_true=df.RealClusters, labels_pred=df.Clusters_k3))\n",
    "\n",
    "print(rand_score(labels_true=df.RealClusters, labels_pred=df.Clusters_k4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('sf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "563b3fbad9c1e703622b628cf34b11b58769da99b1ed9a9df4fdabd54b844cdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
